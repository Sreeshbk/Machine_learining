{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaS01H4iO9TR7uiuMnGi1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreeshbk/Machine_learning/blob/main/concept/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "Regression is a supervised machine learning technique that helps in predicting continuous numerical values or quantity. \n",
        "Regression Model can be a linear or a non-linear function.\n",
        "\n",
        "Regression analysis is a statistical process for estimating the relationships between variables. It can be used to build a model to predict the value of the target variable from the predictor variables.\n",
        "\n",
        "1. Simple Linear Regression model.\n",
        "\n",
        "  $f(X) = ß_0 + ß_1x_1 + ∈$\n",
        "\n",
        "2. Multiple Linear Regression model.\n",
        "\n",
        "  $f(X) = ß_0 + ß_1x_1 + ß_2x_2 + ... + ß_nx_n + ∈$\n",
        "\n",
        "The goal of linear regression is to create a model that predicts the value accurately and consequently has the lowest sum of squared errors (also known as least squares). Such a model is called as the **best fit model**.\n",
        "\n",
        "for equation $y=b_0 +b_1x$,\n",
        "\n",
        "$b_1 = \\frac{[\\sum_{i=0}^n (x_iy_i)] - n\\bar{x}\\bar{y}}{[\\sum_{i=0}^n (x_i^2)] - n\\bar{x}^2}$\n",
        "\n",
        "$b_0 = \\bar{y} + b_1\\bar{x} $"
      ],
      "metadata": {
        "id": "4hf6hjGdDcEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessing the Regression Model\n",
        "\n",
        "1) Assumptions about the form of the model\n",
        "\n",
        "The linear regression model Y = ß0 + ß1x1 + ß2x2 + ... + ßnxn + ∈\n",
        "\n",
        "that relates the response Y to the predictors x1, x2...xn,  is assumed to be linear in the regression coefficients ß0, ß1....ßn , provided that if the relationship between the dependent and predictor variable(s) of the model is linear.\n",
        "\n",
        "2) Assumptions about the errors\n",
        "\n",
        "The errors are assumed to be normally distributed with mean zero and a common variance\n",
        "\n",
        "This implies four assumptions:\n",
        "\n",
        "- The errors (also called as residues/residuals) of the model are normally distributed.\n",
        "\n",
        "- The errors of the model have a mean of zero.\n",
        "\n",
        "- The errors of the model have same variance. This is also referred to as homoscedasticity principle.\n",
        "\n",
        "- The errors of the model should be statistically independent of each other.\n",
        "\n",
        "\n",
        "3) Assumptions about the predictors\n",
        "\n",
        "The predictor variables x1, x2....xn are assumed to be linearly independent of each other. If this assumption is violated then the problem is referred to as the collinearity problem. Test of independence to determine collinearity is discussed in the multiple linear regression section."
      ],
      "metadata": {
        "id": "jolwQv4sW-7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual plot\n",
        "\n",
        "The assumption of linearity can also be validated using the residuals (errors) plotted against the fitted values. The fitted values are the predicted values of the dependent variable. \n",
        "\n",
        "If the residual vs. fitted values plot exhibits any pattern then the relationship may be non-linear.\n",
        "\n",
        "Note:  the residplot() function available in Seaborn is used to get the residuals vs. fitted value plot."
      ],
      "metadata": {
        "id": "ps0PuNqDX_-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scale-location plot. \n",
        "The scale-location plot depicts the square rooted standardized residual vs. predicted value obtained using the best fit model. Standardized residuals are residuals scaled such that they have a mean of 0 and variance of 1.\n",
        "The linear regression model is said to abide by the homoscedasticity assumption if there is no specific pattern observed in the scale-location plot\n",
        "\n",
        "In general, the homoscedasticity is said to be violated if:\n",
        "\n",
        "- The residuals seem to increase or decrease in average magnitude with the fitted values, it is an indication that the variance of the residuals is not constant.\n",
        "\n",
        "- The points in the plot lie on a curve around zero, rather than fluctuating randomly.\n",
        "\n",
        "- A few points in the plot lie a long way from the rest of the points."
      ],
      "metadata": {
        "id": "whOnOOhmY2dE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coefficient of Determination:\n",
        "\n",
        "$R^2$ = $\\frac {sum of squared regression(SSR)}{sum of squared total(SST)}$\n",
        "\n",
        "SST is the sum of squared differences between actual and mean target values,  i.e.\n",
        "\n",
        "$SSR = \\sum(y-\\bar{y})^2$\n",
        "\n",
        "$SST = \\sum(\\hat{y}-\\bar{y})^2$\n",
        "\n",
        "$SST = SSR + SSE$. \n",
        "\n",
        "$SSE = \\sum(\\hat{y}-y)^2$\n",
        "\n",
        "$R^2$ = 1 - $\\frac {sum of squared error(SSE)}{sum of squared total(SST)}$\n",
        "\n",
        "where,\n",
        "\n",
        "$\\bar{y} - mean $\n",
        "\n",
        "$\\hat{y} - predicted $\n",
        "\n",
        "SST can also be expressed in terms of the sum of squared regression (SSR) and the sum of squared errors (SSE) as,\n",
        "\n",
        "\n",
        "\n",
        "A value of 1.0 indicates a perfect fit, and is thus a highly reliable model for future forecasts, while a value of 0.0 would indicate that the calculation fails to accurately model the data at all\n",
        "\n",
        "```np.corrcoef(delivery[\"n.prod\"],delivery[\"distance\"])```\n",
        " "
      ],
      "metadata": {
        "id": "zRU0N_tcIDaw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evma4QE3_Yhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusted R-squared\n",
        "To establish a best fit linear regression model with minimum error the least squares method is used. For a linear regression model, every additional predictor variable tends to minimize the error of the model. As a result, the R2 value will never decrease for any number of additional predictor variables that is included in the model.\n",
        "\n",
        "the R2 value can be inflated by including more and more predictor variables.\n",
        "\n",
        "Thus, the use of an additional statistic known as adjusted R2 is suggested. The adjusted R2 takes into account the number of predictor variables and the number of samples or observations included in the Regression model.\n",
        "\n",
        "The adjusted R2 is defined as: \n",
        "\n",
        "$R^2$ = 1 - $\\frac {SSE/(n-k-1)}{SST/(n-1)}$\n",
        "\n",
        "Where, n is the number of observations and k is the number of number of predictor variables in the model."
      ],
      "metadata": {
        "id": "itDznw6bS_1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multicollinearity \n",
        "is a statistical concept where several independent variables in a model are correlated. Two variables are considered to be perfectly collinear if their correlation coefficient is +/- 1.0\n",
        "\n",
        "#### Variance Inflation Factor\n",
        "\n",
        "In addition to correlation, there is another measure called variance inflation factor(VIF) to determine if the predictor variables are independent of each other.\n",
        "\n",
        "$VIF_i = \\frac{1}{1-R_i^2}$\n",
        "\n",
        "The range of VIF values start from 1. As a common practice, VIF values can be interpreted as follows -\n",
        "\n",
        "- 1 => No correlation between variables\n",
        "- 1 to 5 => Slightly correlated\n",
        "- Greater than 5 => Highly correlated\n",
        "```\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "#calculating the VIF for each attributes\n",
        "vif = pd.Series([variance_inflation_factor(X.values,idx) \n",
        "           for idx in range(X.shape[1])],\n",
        "          index=X.columns)\n",
        "print(vif)\n",
        "```"
      ],
      "metadata": {
        "id": "ymxbM3ogRDHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI15TDyzDQTm"
      },
      "outputs": [],
      "source": []
    }
  ]
}
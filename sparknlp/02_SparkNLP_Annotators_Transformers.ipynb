{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpJnNa50jNaR13zpUvb1xH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreeshbk/Machine_learning/blob/main/sparknlp/02_SparkNLP_Annotators_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing with Spark NLP\n"
      ],
      "metadata": {
        "id": "D39er5Uep49T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Setup"
      ],
      "metadata": {
        "id": "Utq965oyqEDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gwrgiXwkpgUY"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark==3.3.0  spark-nlp==4.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotators and Transformer Concepts\n",
        "In Spark NLP, all Annotators are either Estimators or Transformers \n",
        "- An Estimator in Spark ML is an algorithm which can be fit on a DataFrame to produce a Transformer.<br> E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
        "- A Transformer is an algorithm which can transform one DataFrame into another DataFrame. <br> E.g., an ML model is a Transformer that transforms a DataFrame with features into a DataFrame with predictions. \n",
        "\n",
        "In Spark NLP, there are two types of annotators: \n",
        "1. AnnotatorApproach extends Estimators from Spark ML, which are meant to be trained through fit(), and \n",
        "2. AnnotatorModel extends Transformers which are meant to transform data frames through transform(). \n",
        "\n",
        "Some of Spark NLP annotators have a Model suffix and some do not. The model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers but do not contain the suffix Model since they are not trained, annotators. Model annotators have a pre-trained() on its static object, to retrieve the public pre-trained version of a model. Long story short, if it trains on a DataFrame and produces a model, it’s an AnnotatorApproach; and if it transforms one DataFrame into another DataFrame through some models, it’s an AnnotatorModel (e.g. WordEmbeddingsModel) and it doesn’t take Model suffix if it doesn’t rely on a pre-trained annotator while transforming a DataFrame (e.g. Tokenizer).\n",
        "\n",
        "By convention, there are three possible names:\n",
        "\n",
        "- Approach — Trainable annotator\n",
        "- Model — Trained annotator\n",
        "- nothing — Either a non-trainable annotator with pre-processing step or shorthand for a model\n",
        "\n",
        "So for example, Stemmer doesn’t say Approach nor Model, however, it is a Model. On the other hand, Tokenizer doesn’t say Approach nor Model, but it has a TokenizerModel(). Because it is not “training” anything, but it is doing some preprocessing before converting into a Model. When in doubt, please refer to official documentation and API reference. Even though we will do many hands-on practices in the following articles, let us give you a glimpse to let you understand the difference between AnnotatorApproach and AnnotatorModel. As stated above, Tokenizer is an AnnotatorModel. So we need to call fit() and then transform().\n",
        "\n",
        "Now let’s see how this can be done in Spark NLP using Annotators and Transformers. Assume that we have the following steps that need to be applied one by one on a data frame.\n",
        "\n",
        "- Split text into sentences\n",
        "- Tokenize\n",
        "- Normalize\n",
        "- Get word embeddings"
      ],
      "metadata": {
        "id": "DzxjYOXaqRPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(\"\"))\n",
        "\n",
        "\n",
        "spark = sparknlp.start()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "tjOuR53yscds",
        "outputId": "f972ada6-6236-4a74-b1d7-99fd94e3637b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  4.2.0\n",
            "Apache Spark version:  3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f79085c7f70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e86ac8d3490f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Peter Parker is a nice guy and lives in New York'\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGJNtIqBsgYd",
        "outputId": "d6492a38-ef93-4a7e-9e74-96264828789f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------+\n",
            "|text                                            |\n",
            "+------------------------------------------------+\n",
            "|Peter Parker is a nice guy and lives in New York|\n",
            "+------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "# if you want to create a spark datafarme from a list of strings\n",
        "\n",
        "text_list = ['Peter Parker is a nice guy and lives in New York.', 'Bruce Wayne is also a nice guy and lives in Gotham City.']\n",
        "\n",
        "spark.createDataFrame(text_list, StringType()).toDF(\"text\").show(truncate=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITRnDirHsj_C",
        "outputId": "50d01652-ca15-45e6-de8c-d192f64f7c16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "|                                                    text|\n",
            "+--------------------------------------------------------+\n",
            "|       Peter Parker is a nice guy and lives in New York.|\n",
            "|Bruce Wayne is also a nice guy and lives in Gotham City.|\n",
            "+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt\")"
      ],
      "metadata": {
        "id": "0T4TMkuNsxMS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(SparkFiles.get(\"sample-sentences-en.txt\")) as f:\n",
        "  print (f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBEZYfmBuDML",
        "outputId": "fb795bb8-4433-459f-f373-8fb45772d3ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peter is a very good person.\n",
            "My life in Russia is very interesting.\n",
            "John and Peter are brothers. However they don't support each other that much.\n",
            "Lucas Nogal Dunbercker is no longer happy. He has a good car though.\n",
            "Europe is very culture rich. There are huge churches! and big houses!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.read.text(SparkFiles.get(\"sample-sentences-en.txt\")).toDF('text')\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB-e0mPcunjZ",
        "outputId": "4e5ee5ec-3f3d-4f22-bdbb-a21a957b274b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Transformers\t| Description |\n",
        "|------------|---------------|\n",
        "|DocumentAssembler |\tTo get through the NLP process, we need to get raw data annotated. This is a special transformer that does this for us; it creates the first annotation of type Document which may be used by annotators down the road.\n",
        "|TokenAssembler\t| This transformer reconstructs a Document type annotation from tokens, usually after these have been, lemmatized, normalized, spell checked, etc, to use this document annotation in further annotators.\n",
        "|Doc2Chunk |\tConverts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol.\n",
        "|Chunk2Doc |\tConverts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result.\n",
        "|Finisher\t| Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n"
      ],
      "metadata": {
        "id": "CgDnxRBbxKGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Document Assembler**"
      ],
      "metadata": {
        "id": "x26St9vwxR8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\\\n",
        "    .setCleanupMode(\"shrink\")\n",
        "\n",
        "doc_df = documentAssembler.transform(spark_df)\n",
        "\n",
        "doc_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EotIy26QveYT",
        "outputId": "2ed1bd98-a0a2-4b65-ce12-5cf2c3574c80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                 |\n",
            "|My life in Russia is very interesting.                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[{document, 0, 76, John and Peter are brothers. However they don't support each other that much., {sentence -> 0}, []}]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]        |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x046cfmCxk3y",
        "outputId": "a94664ce-6a82-4353-db32-250ded4ce266"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df.select('document.result','document.begin','document.end').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qeVeS9vu4Ar",
        "outputId": "c38c0a87-5c63-483f-8f60-4d8e403c3d79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|result                                                                         |begin|end |\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|[Peter is a very good person.]                                                 |[0]  |[27]|\n",
            "|[My life in Russia is very interesting.]                                       |[0]  |[37]|\n",
            "|[John and Peter are brothers. However they don't support each other that much.]|[0]  |[76]|\n",
            "|[Lucas Nogal Dunbercker is no longer happy. He has a good car though.]         |[0]  |[67]|\n",
            "|[Europe is very culture rich. There are huge churches! and big houses!]        |[0]  |[68]|\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "doc_df.withColumn(\n",
        "    \"tmp\", \n",
        "    F.explode(\"document\"))\\\n",
        "    .select(\"tmp.*\")\\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEHnFgqQvp0f",
        "outputId": "a86baddd-13ae-4121-de81-ef323c8f3560"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|annotatorType|begin|end|result                                                                       |metadata       |embeddings|\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|document     |0    |27 |Peter is a very good person.                                                 |{sentence -> 0}|[]        |\n",
            "|document     |0    |37 |My life in Russia is very interesting.                                       |{sentence -> 0}|[]        |\n",
            "|document     |0    |76 |John and Peter are brothers. However they don't support each other that much.|{sentence -> 0}|[]        |\n",
            "|document     |0    |67 |Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |{sentence -> 0}|[]        |\n",
            "|document     |0    |68 |Europe is very culture rich. There are huge churches! and big houses!        |{sentence -> 0}|[]        |\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentence Detector**"
      ],
      "metadata": {
        "id": "OyBLCnYox_Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "# we feed the document column coming from Document Assembler\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "      .setInputCols(['document'])\\\n",
        "      .setOutputCol('sentences')"
      ],
      "metadata": {
        "id": "fnJTZmRSvXHV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceDetector.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chD1V3_gyKXe",
        "outputId": "55c69042-2b97-4252-9c9f-cb84af3ab9b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='SentenceDetector_d6b9bbf26f5f', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='useAbbreviations', doc='whether to apply abbreviations at sentence detection'): True,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='detectLists', doc='whether detect lists during sentence detection'): True,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='useCustomBoundsOnly', doc='Only utilize custom bounds in sentence detection'): False,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='customBounds', doc='characters used to explicitly mark sentence bounds'): [],\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='customBoundsStrategy', doc='How to return matched custom bounds'): 'none',\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='explodeSentences', doc='whether to explode each sentence into a different row, for better parallelization. Defaults to false.'): False,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='minLength', doc='Set the minimum allowed length for each sentence.'): 0,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='maxLength', doc='Set the maximum allowed length for each sentence'): 99999,\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='SentenceDetector_d6b9bbf26f5f', name='outputCol', doc='output annotation column. can be left default.'): 'sentences'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdCSEv2KyQoL",
        "outputId": "13e7fd4a-2a13-4b9f-947e-d7eb5bac4ce8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |sentences                                                                                                                                                                                          |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                 |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                                                                                             |\n",
            "|My life in Russia is very interesting.                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                       |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                                                                                                   |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[{document, 0, 76, John and Peter are brothers. However they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 27, John and Peter are brothers., {sentence -> 0}, []}, {document, 29, 76, However they don't support each other that much., {sentence -> 1}, []}]                                  |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]         |[{document, 0, 41, Lucas Nogal Dunbercker is no longer happy., {sentence -> 0}, []}, {document, 43, 67, He has a good car though., {sentence -> 1}, []}]                                           |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]        |[{document, 0, 27, Europe is very culture rich., {sentence -> 0}, []}, {document, 29, 52, There are huge churches!, {sentence -> 1}, []}, {document, 54, 68, and big houses!, {sentence -> 2}, []}]|\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMK6qqEDybA1",
        "outputId": "31853c08-0164-4263-f90e-e9ccc574ad65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='Peter is a very good person.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=37, result='My life in Russia is very interesting.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='John and Peter are brothers.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='document', begin=29, end=76, result=\"However they don't support each other that much.\", metadata={'sentence': '1'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNTwZIhayeQs",
        "outputId": "95789662-f522-4ebc-cc8e-2bde8d4c2d75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.']),\n",
              " Row(result=['My life in Russia is very interesting.']),\n",
              " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.metadata').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksvc1tYZyjEm",
        "outputId": "07603ad8-40fd-4b0f-9b6b-fc7a274c9917"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(metadata=[{'sentence': '0'}]),\n",
              " Row(metadata=[{'sentence': '0'}]),\n",
              " Row(metadata=[{'sentence': '0'}, {'sentence': '1'}])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text ='The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'\n",
        "text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wgJowGjTyl1U",
        "outputId": "148adbf4-9412-4eb4-d533-75177bfdc338"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkXPajlVyo0H",
        "outputId": "c97396da-adbe-4e00-c0d8-de50f625ae0f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                           |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = documentAssembler.transform(spark_df)\n",
        "\n",
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "\n",
        "sent_df.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06rN_29eytQl",
        "outputId": "3cda4b0c-4f0a-4cbf-de6a-a596bb248c97"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                                          document|                                         sentences|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 0, 56, The patient was prescribed 1...|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setExplodeSentences: Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.\n",
        "\n",
        "sentenceDetector.setExplodeSentences(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_1fYveSy0V1",
        "outputId": "f54aa4aa-0468-4bc4-8f1c-54bfa2e9bb6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceDetector_d6b9bbf26f5f"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "\n",
        "sent_df.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srBvhRHAy4yw",
        "outputId": "9028291b-d1c8-4e1b-e43d-15eb584cfb28"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                                          document|                                         sentences|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 0, 56, The patient was prescribed 1...|\n",
            "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 58, 240, He was seen by the endocri...|\n",
            "|The patient was prescribed 1 capsule of Advil f...|[{document, 0, 334, The patient was prescribed ...|[{document, 242, 334, It was determined that al...|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNMkQBjTy_k6",
        "outputId": "70b2cbaf-5c48-48e2-dcca-ea462884cdb5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[The patient was prescribed 1 capsule of Advil for 5 days.]                                                                                                                              |\n",
            "|[He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.]|\n",
            "|[It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.]                                                                                          |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "sent_df.select(F.explode('sentences.result')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKadM41SzCDf",
        "outputId": "14c0fa6c-67b9-4ef3-bfac-d5f822b51c24"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|col                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
            "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
            "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months.                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    [\"Peter is a very good person.\"],\n",
        "    [\"My life in Russia is very interesting.\"], \n",
        "    [\"John and Peter are brothers. However; they don't support each other that much.\"],\n",
        "    [\"Lucas Nogal Dunbercker is no longer happy. He has a good car though.\"],\n",
        "    [\"Europe is very culture rich. There are huge churches! and big houses!\"]\n",
        "    ]\n",
        "spark_df = spark.createDataFrame(text).toDF(\"text\")\n",
        "spark_df.show(truncate=False)\n",
        "doc_df = documentAssembler.transform(spark_df)\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "      .setInputCols(['document'])\\\n",
        "      .setOutputCol('sentences')\\\n",
        "      .setCustomBounds([r\"\\.\", \";\", \"!\"])\\\n",
        "      .setCustomBoundsStrategy(\"append\")\n",
        "      \n",
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZlg2S4LzPHJ",
        "outputId": "df2f52db-f692-4981-e264-66dc5e5e0de3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------+\n",
            "|text                                                                          |\n",
            "+------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                  |\n",
            "|My life in Russia is very interesting.                                        |\n",
            "|John and Peter are brothers. However; they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!         |\n",
            "+------------------------------------------------------------------------------+\n",
            "\n",
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                          |document                                                                                                                |sentences                                                                                                                                                                                                   |\n",
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                                                                                                      |\n",
            "|My life in Russia is very interesting.                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                                                                                                            |\n",
            "|John and Peter are brothers. However; they don't support each other that much.|[{document, 0, 77, John and Peter are brothers. However; they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 27, John and Peter are brothers., {sentence -> 0}, []}, {document, 29, 36, However;, {sentence -> 1}, []}, {document, 38, 77, they don't support each other that much., {sentence -> 2}, []}]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]          |[{document, 0, 41, Lucas Nogal Dunbercker is no longer happy., {sentence -> 0}, []}, {document, 43, 67, He has a good car though., {sentence -> 1}, []}]                                                    |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!         |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]         |[{document, 0, 27, Europe is very culture rich., {sentence -> 0}, []}, {document, 29, 52, There are huge churches!, {sentence -> 1}, []}, {document, 54, 68, and big houses!, {sentence -> 2}, []}]         |\n",
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH2njSL5zhej",
        "outputId": "24eaa0d6-981d-49c1-c98e-625b38ade5ff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.']),\n",
              " Row(result=['My life in Russia is very interesting.']),\n",
              " Row(result=['John and Peter are brothers.', 'However;', \"they don't support each other that much.\"]),\n",
              " Row(result=['Lucas Nogal Dunbercker is no longer happy.', 'He has a good car though.']),\n",
              " Row(result=['Europe is very culture rich.', 'There are huge churches!', 'and big houses!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceDetector = SentenceDetector()\\\n",
        "      .setInputCols(['document'])\\\n",
        "      .setOutputCol('sentences')\\\n",
        "      .setCustomBounds([r\"\\.\", \";\", \"!\"])\\\n",
        "      .setCustomBoundsStrategy(\"prepend\")\n",
        "\n",
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYwpyEnGzlN3",
        "outputId": "8ffc1717-235a-48c3-e7d0-e2be0c83827e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                          |document                                                                                                                |sentences                                                                                                                                                                                                                                                                                                                                    |\n",
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                  |[{document, 0, 27, Peter is a very good person., {sentence -> 0}, []}]                                                  |[{document, 0, 26, Peter is a very good person, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}]                                                                                                                                                                                                                            |\n",
            "|My life in Russia is very interesting.                                        |[{document, 0, 37, My life in Russia is very interesting., {sentence -> 0}, []}]                                        |[{document, 0, 36, My life in Russia is very interesting, {sentence -> 0}, []}, {document, 37, 37, ., {sentence -> 1}, []}]                                                                                                                                                                                                                  |\n",
            "|John and Peter are brothers. However; they don't support each other that much.|[{document, 0, 77, John and Peter are brothers. However; they don't support each other that much., {sentence -> 0}, []}]|[{document, 0, 26, John and Peter are brothers, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}, {document, 29, 35, However, {sentence -> 2}, []}, {document, 36, 36, ;, {sentence -> 3}, []}, {document, 38, 76, they don't support each other that much, {sentence -> 4}, []}, {document, 77, 77, ., {sentence -> 5}, []}]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.          |[{document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., {sentence -> 0}, []}]          |[{document, 0, 40, Lucas Nogal Dunbercker is no longer happy, {sentence -> 0}, []}, {document, 41, 41, ., {sentence -> 1}, []}, {document, 43, 66, He has a good car though, {sentence -> 2}, []}, {document, 67, 67, ., {sentence -> 3}, []}]                                                                                               |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!         |[{document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, {sentence -> 0}, []}]         |[{document, 0, 26, Europe is very culture rich, {sentence -> 0}, []}, {document, 27, 27, ., {sentence -> 1}, []}, {document, 29, 51, There are huge churches, {sentence -> 2}, []}, {document, 52, 52, !, {sentence -> 3}, []}, {document, 54, 67, and big houses, {sentence -> 4}, []}, {document, 68, 68, !, {sentence -> 5}, []}]         |\n",
            "+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The separation of the sentences is determined according to the characters we set with custom bound. When we use `append`, sentences are differentiated according to the characters, if `prepend` is used, it also determines the characters as separate sentences."
      ],
      "metadata": {
        "id": "VUucUUQHzuZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.select('sentences.result').take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca8ZUTAezwIz",
        "outputId": "eded2ce6-84e4-4968-f5a8-b33163bcbc65"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person', '.']),\n",
              " Row(result=['My life in Russia is very interesting', '.']),\n",
              " Row(result=['John and Peter are brothers', '.', 'However', ';', \"they don't support each other that much\", '.']),\n",
              " Row(result=['Lucas Nogal Dunbercker is no longer happy', '.', 'He has a good car though', '.']),\n",
              " Row(result=['Europe is very culture rich', '.', 'There are huge churches', '!', 'and big houses', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Detector DL"
      ],
      "metadata": {
        "id": "pzxCsgCOz_7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text ='The patient was prescribed 1 capsule of Advil for 5 days. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day. It was determined that all SGLT2 inhibitors should be discontinued indefinitely from 3 months.'\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "doc_df = documentAssembler.transform(spark_df)"
      ],
      "metadata": {
        "id": "BcmYcnA70FRt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentences\")\\\n",
        "\n",
        "sent_dl_df = sentencerDL.transform(doc_df)\n",
        "\n",
        "sent_dl_df.select(F.explode('sentences.result')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocHL_t0V0Jq_",
        "outputId": "789559ef-8b06-4d6b-eef6-2019f406db11"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|col                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days.                                                                                                                              |\n",
            "|He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.|\n",
            "|It was determined that all SGLT2 inhibitors should be discontinued indefinitely from 3 months.                                                                                         |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentences')\\\n",
        "    \n",
        "sentencerDL = SentenceDetectorDLModel\\\n",
        "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentences\")\n",
        "\n",
        "\n",
        "sd_pipeline = PipelineModel(stages=[documenter, sentenceDetector])\n",
        "sd_model = LightPipeline(sd_pipeline)\n",
        "\n",
        "\n",
        "# DL version\n",
        "sd_dl_pipeline = PipelineModel(stages=[documenter, sentencerDL])\n",
        "sd_dl_model = LightPipeline(sd_dl_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8seStMVb0OfP",
        "outputId": "0991c0ac-4a8e-40b1-a7be-d53c8cb5379e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"John loves Mary.Mary loves Peter\n",
        "Peter loves Helen .Helen loves John; \n",
        "Total: four people involved.\"\"\"\n",
        "\n",
        "# sd_model\n",
        "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBPrL_rl0aA2",
        "outputId": "bc2399d9-4b04-4484-d229-bce7f6e1a886"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t51\tJohn loves Mary.Mary loves Peter\n",
            "Peter loves Helen .\n",
            "1\t52\t68\tHelen loves John;\n",
            "2\t71\t98\tTotal: four people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sd_dl_model\n",
        "for anno in sd_dl_model.fullAnnotate(text)[0][\"sentences\"]:\n",
        "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
        "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRfgs8WF0kr-",
        "outputId": "9c77fe0d-1da9-4ddb-ab53-b68344dab444"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t15\tJohn loves Mary.\n",
            "1\t16\t31\tMary loves Peter\n",
            "2\t33\t51\tPeter loves Helen .\n",
            "3\t52\t68\tHelen loves John;\n",
            "4\t71\t98\tTotal: four people involved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "Identifies tokens with tokenization open standards. It is an Annotator Approach, so it requires .fit().\n",
        "\n",
        "A few rules will help customizing it if defaults do not fit user needs."
      ],
      "metadata": {
        "id": "m8PR9Kl50BsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "J_H20qxM1OJM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxTDz-SO1Q_M",
        "outputId": "9d1cd58c-70c1-495e-ffef-320681daa374"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='Tokenizer_283ea89ee820', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='Tokenizer_283ea89ee820', name='targetPattern', doc='pattern to grab from text as token candidates. Defaults \\\\S+'): '\\\\S+',\n",
              " Param(parent='Tokenizer_283ea89ee820', name='contextChars', doc='character list used to separate from token boundaries'): ['.',\n",
              "  ',',\n",
              "  ';',\n",
              "  ':',\n",
              "  '!',\n",
              "  '?',\n",
              "  '*',\n",
              "  '-',\n",
              "  '(',\n",
              "  ')',\n",
              "  '\"',\n",
              "  \"'\"],\n",
              " Param(parent='Tokenizer_283ea89ee820', name='caseSensitiveExceptions', doc='Whether to care for case sensitiveness in exceptions'): True,\n",
              " Param(parent='Tokenizer_283ea89ee820', name='minLength', doc='Set the minimum allowed length for each token'): 0,\n",
              " Param(parent='Tokenizer_283ea89ee820', name='maxLength', doc='Set the maximum allowed length for each token'): 99999,\n",
              " Param(parent='Tokenizer_283ea89ee820', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='Tokenizer_283ea89ee820', name='outputCol', doc='output annotation column. can be left default.'): 'token'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n",
        "\n",
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")"
      ],
      "metadata": {
        "id": "OLPqYfUd1Whm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_df = documentAssembler.transform(spark_df)\n",
        "\n",
        "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
        "\n",
        "token_df.show(truncate=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9pqwnrY1ZHX",
        "outputId": "c9959e01-5755-4996-b188-03a45c5bd023"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                           text|                                                                                            document|                                                                                               token|\n",
            "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!|[{document, 0, 78, Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail...|[{token, 0, 4, Peter, {sentence -> 0}, []}, {token, 6, 11, Parker, {sentence -> 0}, []}, {token, ...|\n",
            "+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_df.select('token.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P85CMIev2DV-",
        "outputId": "07389a31-f3c0-4e88-f9e8-23103172f70a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(', 'Spiderman', ')', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New', 'York', 'but', 'has', 'no', 'e-mail', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\") \\\n",
        "    .setSplitChars(['-']) \\\n",
        "    .setContextChars(['?', '!']) \\\n",
        "    .addException(\"New York\") "
      ],
      "metadata": {
        "id": "hUfMKQw12HLg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
        "\n",
        "token_df.select('token.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLcKyGXf2J1-",
        "outputId": "2146e465-6e9b-4498-8c94-76fa25e26a92"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e', 'mail', '!'])]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexTokenizer"
      ],
      "metadata": {
        "id": "QFXKL_LlyzXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%\"\n",
        "pattern = \"\\\\s+|(?=[-.:;*+,$&%\\\\[\\\\]])|(?<=[-.:;*+,$&%\\\\[\\\\]])\"\n",
        "\n",
        "df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n",
        "\n",
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "regexTokenizer = RegexTokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"regexToken\") \\\n",
        "    .setPattern(pattern) \\\n",
        "    .setPositionalMask(False)\n",
        "\n",
        "docPatternRemoverPipeline = Pipeline().setStages([documenter,\n",
        "                                                  sentenceDetector,\n",
        "                                                  regexTokenizer])\n",
        "\n",
        "result = docPatternRemoverPipeline.fit(df).transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCCfe_he20ei",
        "outputId": "31476278-efa7-41a5-ad69-c59ec5e3a4ba"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                               |document                                                                                     |sentence                                                                                     |regexToken                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{document, 0, 50, 1. T1-T2 DATE**[12/24/13] $1.99 () (10/12), ph+ 90%, {sentence -> 0}, []}]|[{token, 0, 0, 1, {sentence -> 0}, []}, {token, 2, 2, ., {sentence -> 0}, []}, {token, 4, 5, T1, {sentence -> 0}, []}, {token, 7, 7, -, {sentence -> 0}, []}, {token, 9, 10, T2, {sentence -> 0}, []}, {token, 12, 15, DATE, {sentence -> 0}, []}, {token, 17, 17, *, {sentence -> 0}, []}, {token, 19, 19, *, {sentence -> 0}, []}, {token, 21, 21, [, {sentence -> 0}, []}, {token, 23, 30, 12/24/13, {sentence -> 0}, []}, {token, 32, 32, ], {sentence -> 0}, []}, {token, 35, 35, $, {sentence -> 0}, []}, {token, 37, 37, 1, {sentence -> 0}, []}, {token, 39, 39, ., {sentence -> 0}, []}, {token, 41, 42, 99, {sentence -> 0}, []}, {token, 44, 45, (), {sentence -> 0}, []}, {token, 47, 53, (10/12), {sentence -> 0}, []}, {token, 55, 55, ,, {sentence -> 0}, []}, {token, 57, 58, ph, {sentence -> 0}, []}, {token, 60, 60, +, {sentence -> 0}, []}, {token, 62, 63, 90, {sentence -> 0}, []}, {token, 65, 65, %, {sentence -> 0}, []}]|\n",
            "+---------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result_df = result.select(F.explode(result.regexToken.result).alias('regexToken')).toPandas()\n",
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "OVGTGUII3PNG",
        "outputId": "77acd796-1308-438d-fa1d-29b6902cb9ab"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   regexToken\n",
              "0           1\n",
              "1           .\n",
              "2          T1\n",
              "3           -\n",
              "4          T2\n",
              "5        DATE\n",
              "6           *\n",
              "7           *\n",
              "8           [\n",
              "9    12/24/13\n",
              "10          ]\n",
              "11          $\n",
              "12          1\n",
              "13          .\n",
              "14         99\n",
              "15         ()\n",
              "16    (10/12)\n",
              "17          ,\n",
              "18         ph\n",
              "19          +\n",
              "20         90\n",
              "21          %"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-40f3385d-00d7-4f93-8116-123af636d88f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>regexToken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>T2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12/24/13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>()</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(10/12)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40f3385d-00d7-4f93-8116-123af636d88f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40f3385d-00d7-4f93-8116-123af636d88f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40f3385d-00d7-4f93-8116-123af636d88f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking Spark NLP Annotators in Spark ML Pipeline\n",
        "Spark NLP provides an easy API to integrate with Spark ML Pipelines and all the Spark NLP annotators and transformers can be used within Spark ML Pipelines. So, it’s better to explain Pipeline concept through Spark ML official documentation.\n",
        "\n",
        "What is a Pipeline anyway? In machine learning, it is common to run a sequence of algorithms to process and learn from data.\n",
        "\n",
        "Apache Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order.\n",
        "\n",
        "In simple terms, a pipeline chains multiple Transformers and Estimators together to specify an ML workflow. We use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow.\n",
        "\n",
        "The figure below is for the training time usage of a Pipeline."
      ],
      "metadata": {
        "id": "t5MouJ023WL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentences\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               sentenceDetector,\n",
        "                               tokenizer])\n",
        "\n",
        "spark_df = spark.read.text(SparkFiles.get('sample-sentences-en.txt')).toDF('text')\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(spark_df)"
      ],
      "metadata": {
        "id": "k35TEkYT3roR"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelineModel.transform(spark_df)"
      ],
      "metadata": {
        "id": "sMK5zLQ_37Xk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.show(truncate=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVzKc68x3-a5",
        "outputId": "43424612-e21a-4646-826c-241e334a2087"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                                    text|                                document|                               sentences|                                   token|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|\n",
            "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|\n",
            "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{document, 0, 27, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|\n",
            "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{document, 0, 41, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|\n",
            "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{document, 0, 27, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aH4C3Hc4BPe",
        "outputId": "22385418-e3c6-4ce2-b48b-872084ad1d21"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- sentences: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('sentences.result').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVmYNnMV4Fd7",
        "outputId": "78808d71-4dbb-403f-fa00-5557668279fd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.']),\n",
              " Row(result=['My life in Russia is very interesting.']),\n",
              " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token').take(3)[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtSbpS2I4IGL",
        "outputId": "4ee72aa1-f8d0-4f6f-f2d9-05df27ff5bed"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(token=[Row(annotatorType='token', begin=0, end=3, result='John', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='However', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=42, end=46, result=\"don't\", metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=76, end=76, result='.', metadata={'sentence': '1'}, embeddings=[])])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizer\n",
        "Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "MLqYTBDv4MB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vYidA8Mj4QS8",
        "outputId": "4c1bd2eb-7075-4a35-c3b1-261e83abf510"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "    \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(True)\\\n",
        "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
        "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               normalizer])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)"
      ],
      "metadata": {
        "id": "UXvMvOfB4Uuf"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlpPipeline.fit(spark_df).stages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H06ZbxB_4iY0",
        "outputId": "dfb1b47e-a960-4dc6-a0fc-6169f87b5bf9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DocumentAssembler_4d99c9cf74d2,\n",
              " REGEX_TOKENIZER_3e600fbfdfd5,\n",
              " NORMALIZER_dccfa12df1af]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.show(truncate=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fssC10S_4upR",
        "outputId": "5e531117-f949-4035-898e-e522930535fa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                                    text|                                document|                                   token|                              normalized|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, peter, {sentence -> 0}...|\n",
            "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 0, 1, my, {sentence -> 0}, [...|\n",
            "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, john, {sentence -> 0},...|\n",
            "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, lucas, {sentence -> 0}...|\n",
            "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, europe, {sentence -> 0...|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token').take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWVdD69M4zX5",
        "outputId": "c13ecf3e-bd47-4810-8d9c-ac17f8d91915"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(token=[Row(annotatorType='token', begin=0, end=4, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(token=[Row(annotatorType='token', begin=0, end=1, result='My', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='Russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=37, end=37, result='.', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('normalized.result').take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWBhxPPR42ZB",
        "outputId": "6c332be9-4d35-4f5c-d85e-5023c70360f8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['peter', 'is', 'a', 'very', 'good', 'person']),\n",
              " Row(result=['my', 'life', 'in', 'russia', 'is', 'very', 'interesting'])]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('normalized').take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzzRFUrO46i3",
        "outputId": "fe03c546-aeaf-4ecc-d554-e8f079c4ce77"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(normalized=[Row(annotatorType='token', begin=0, end=4, result='peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=6, end=7, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=9, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=14, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=16, end=19, result='good', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=26, result='person', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(normalized=[Row(annotatorType='token', begin=0, end=1, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=3, end=6, result='life', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=8, end=9, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='russia', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=24, result='very', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=36, result='interesting', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Normalizer\n",
        "The DocumentNormalizer is an annotator that can be used after the DocumentAssembler to normalize documents once that they have been processed and indexed . It takes in input annotated documents of type Array AnnotatorType.DOCUMENT and gives as output annotated document of type AnnotatorType.DOCUMENT ."
      ],
      "metadata": {
        "id": "cu1HSULY4_Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\n",
        "    THE WORLD'S LARGEST WEB DEVELOPER SITE\n",
        "    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\n",
        "    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\n",
        "  </div>\n",
        "\n",
        "</div>'''"
      ],
      "metadata": {
        "id": "xmrp72kd5HDA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "spark_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lEnGHCD5KR2",
        "outputId": "e9ea9173-6087-437d-9633-571346878d81"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|\\n  <div id=\"theworldsgreatest\" class='my-right my-hide-small my-wide toptext' style=\"font-family:'Segoe UI',Arial,sans-serif\">\\n    THE WORLD'S LARGEST WEB DEVELOPER SITE\\n    <h1 style=\"font-size:300%;\">THE WORLD'S LARGEST WEB DEVELOPER SITE</h1>\\n    <p style=\"font-size:160%;\">Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..</p>\\n  </div>\\n\\n</div>|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\")\n",
        "\n",
        "documentNormalizer.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwXabg9e5NEL",
        "outputId": "04d2d8dd-a190-4df0-cb3b-c84d5660d65f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='DocumentNormalizer_ad461b66111f', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='action', doc='action to perform applying regex patterns on text'): 'clean',\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='patterns', doc='normalization regex patterns which match will be removed from document. Defaults is <[^>]*>'): ['<[^>]*>'],\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='replacement', doc='replacement string to apply when regexes match'): ' ',\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='lowercase', doc='whether to convert strings to lowercase'): False,\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='policy', doc='policy to remove pattern from text'): 'pretty_all',\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='encoding', doc='file encoding to apply on normalized documents'): 'UTF-8',\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='inputCols', doc='previous annotations columns, if renamed'): ['document'],\n",
              " Param(parent='DocumentNormalizer_ad461b66111f', name='outputCol', doc='output annotation column. can be left default.'): 'normalizedDocument'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('text') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "#default\n",
        "cleanUpPatterns = [\"<[^>]*>\"]\n",
        "\n",
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\") \\\n",
        "    .setAction(\"clean\") \\\n",
        "    .setPatterns(cleanUpPatterns) \\\n",
        "    .setReplacement(\" \") \\\n",
        "    .setPolicy(\"pretty_all\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "docPatternRemoverPipeline = Pipeline() \\\n",
        "    .setStages([documentAssembler,\n",
        "                documentNormalizer])\n",
        "    \n",
        "pipelineModel = docPatternRemoverPipeline.fit(spark_df).transform(spark_df)"
      ],
      "metadata": {
        "id": "spcNEd035P_J"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipelineModel.select('normalizedDocument.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCsNHE7D5T3A",
        "outputId": "cdc94a1f-ea1c-4822-c80e-c027a54fa837"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ the world's largest web developer site the world's largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords Cleaner\n",
        "This annotator excludes from a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences."
      ],
      "metadata": {
        "id": "tZaEnkSo5a1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)"
      ],
      "metadata": {
        "id": "3uwuOXra5ccL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_cleaner.getStopWords()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJVuM66d5g83",
        "outputId": "46dbf172-7b2d-4490-b50e-709f6413010a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " 'should',\n",
              " 'now',\n",
              " \"i'll\",\n",
              " \"you'll\",\n",
              " \"he'll\",\n",
              " \"she'll\",\n",
              " \"we'll\",\n",
              " \"they'll\",\n",
              " \"i'd\",\n",
              " \"you'd\",\n",
              " \"he'd\",\n",
              " \"she'd\",\n",
              " \"we'd\",\n",
              " \"they'd\",\n",
              " \"i'm\",\n",
              " \"you're\",\n",
              " \"he's\",\n",
              " \"she's\",\n",
              " \"it's\",\n",
              " \"we're\",\n",
              " \"they're\",\n",
              " \"i've\",\n",
              " \"we've\",\n",
              " \"you've\",\n",
              " \"they've\",\n",
              " \"isn't\",\n",
              " \"aren't\",\n",
              " \"wasn't\",\n",
              " \"weren't\",\n",
              " \"haven't\",\n",
              " \"hasn't\",\n",
              " \"hadn't\",\n",
              " \"don't\",\n",
              " \"doesn't\",\n",
              " \"didn't\",\n",
              " \"won't\",\n",
              " \"wouldn't\",\n",
              " \"shan't\",\n",
              " \"shouldn't\",\n",
              " \"mustn't\",\n",
              " \"can't\",\n",
              " \"couldn't\",\n",
              " 'cannot',\n",
              " 'could',\n",
              " \"here's\",\n",
              " \"how's\",\n",
              " \"let's\",\n",
              " 'ought',\n",
              " \"that's\",\n",
              " \"there's\",\n",
              " \"what's\",\n",
              " \"when's\",\n",
              " \"where's\",\n",
              " \"who's\",\n",
              " \"why's\",\n",
              " 'would']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               stopwords_cleaner])\n",
        " \n",
        "spark_df = spark.read.text(SparkFiles.get('sample-sentences-en.txt')).toDF('text')\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "\n",
        "result.show(truncate=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcKWNVvx5jng",
        "outputId": "83c52fa8-ddc7-4c00-ca82-e0d526c468ad"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                                    text|                                document|                                   token|                             cleanTokens|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, Peter, {sentence -> 0}...|\n",
            "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 3, 6, life, {sentence -> 0},...|\n",
            "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, John, {sentence -> 0},...|\n",
            "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, Lucas, {sentence -> 0}...|\n",
            "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, Europe, {sentence -> 0...|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('cleanTokens.result').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iclq36eU5pZN",
        "outputId": "4bf1970d-9171-4603-fa5e-5d1eb324a043"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'good', 'person', '.'])]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Assembler"
      ],
      "metadata": {
        "id": "G5rCeC5D5x0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentences\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(False)\\\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"normalized\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "\n",
        "tokenassembler = TokenAssembler()\\\n",
        "    .setInputCols([\"sentences\", \"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"clean_text\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
        "                               sentenceDetector,\n",
        "                               tokenizer,\n",
        "                               normalizer,\n",
        "                               stopwords_cleaner,\n",
        "                               tokenassembler])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2VBQlna5z3w",
        "outputId": "9732145e-1ba0-41d2-bc7e-3806a420611b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|           sentences|               token|          normalized|         cleanTokens|          clean_text|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[{document, 0, 27...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, Pe...|[{token, 0, 4, Pe...|[{document, 0, 16...|\n",
            "|My life in Russia...|[{document, 0, 37...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, My...|[{token, 3, 6, li...|[{document, 0, 22...|\n",
            "|John and Peter ar...|[{document, 0, 76...|[{document, 0, 27...|[{token, 0, 3, Jo...|[{token, 0, 3, Jo...|[{token, 0, 3, Jo...|[{document, 0, 18...|\n",
            "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{document, 0, 41...|[{token, 0, 4, Lu...|[{token, 0, 4, Lu...|[{token, 0, 4, Lu...|[{document, 0, 34...|\n",
            "|Europe is very cu...|[{document, 0, 68...|[{document, 0, 27...|[{token, 0, 5, Eu...|[{token, 0, 5, Eu...|[{token, 0, 5, Eu...|[{document, 0, 18...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('clean_text').take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVgZq5ZY57BS",
        "outputId": "edd0bf0a-31f6-4efb-e214-27d91b994907"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(clean_text=[Row(annotatorType='document', begin=0, end=16, result='Peter good person', metadata={'sentence': '0'}, embeddings=[])])]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('text', F.explode(result.clean_text.result).alias('clean_text')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReQzjEd25-mm",
        "outputId": "5e4862d5-eaeb-4d81-a86c-033565daf8e0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "|text                                                                         |clean_text                         |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "|Peter is a very good person.                                                 |Peter good person                  |\n",
            "|My life in Russia is very interesting.                                       |life Russia interesting            |\n",
            "|John and Peter are brothers. However they don't support each other that much.|John Peter brothers                |\n",
            "|John and Peter are brothers. However they don't support each other that much.|However dont support much          |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |Lucas Nogal Dunbercker longer happy|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |good car though                    |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |Europe culture rich                |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |huge churches                      |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |big houses                         |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('text', F.explode(result.clean_text.result).alias('clean_text')).toPandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "0qAOfJsu6IBy",
        "outputId": "a7fae81d-89f7-456c-bee3-aefae52b0162"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0                       Peter is a very good person.   \n",
              "1             My life in Russia is very interesting.   \n",
              "2  John and Peter are brothers. However they don'...   \n",
              "3  John and Peter are brothers. However they don'...   \n",
              "4  Lucas Nogal Dunbercker is no longer happy. He ...   \n",
              "5  Lucas Nogal Dunbercker is no longer happy. He ...   \n",
              "6  Europe is very culture rich. There are huge ch...   \n",
              "7  Europe is very culture rich. There are huge ch...   \n",
              "8  Europe is very culture rich. There are huge ch...   \n",
              "\n",
              "                            clean_text  \n",
              "0                    Peter good person  \n",
              "1              life Russia interesting  \n",
              "2                  John Peter brothers  \n",
              "3            However dont support much  \n",
              "4  Lucas Nogal Dunbercker longer happy  \n",
              "5                      good car though  \n",
              "6                  Europe culture rich  \n",
              "7                        huge churches  \n",
              "8                           big houses  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-039c9f60-5f54-4afa-8b5d-12b90bd9882a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter is a very good person.</td>\n",
              "      <td>Peter good person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My life in Russia is very interesting.</td>\n",
              "      <td>life Russia interesting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>John and Peter are brothers. However they don'...</td>\n",
              "      <td>John Peter brothers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>John and Peter are brothers. However they don'...</td>\n",
              "      <td>However dont support much</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lucas Nogal Dunbercker is no longer happy. He ...</td>\n",
              "      <td>Lucas Nogal Dunbercker longer happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lucas Nogal Dunbercker is no longer happy. He ...</td>\n",
              "      <td>good car though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Europe is very culture rich. There are huge ch...</td>\n",
              "      <td>Europe culture rich</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Europe is very culture rich. There are huge ch...</td>\n",
              "      <td>huge churches</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Europe is very culture rich. There are huge ch...</td>\n",
              "      <td>big houses</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-039c9f60-5f54-4afa-8b5d-12b90bd9882a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-039c9f60-5f54-4afa-8b5d-12b90bd9882a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-039c9f60-5f54-4afa-8b5d-12b90bd9882a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result.withColumn(\n",
        "    \"tmp\", \n",
        "    F.explode(\"clean_text\")) \\\n",
        "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_avkLP46LDq",
        "outputId": "4c57adab-0a9a-45f7-a47b-8460cf4f8437"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----------------------------------+--------+\n",
            "|begin|end|result                             |sentence|\n",
            "+-----+---+-----------------------------------+--------+\n",
            "|0    |16 |Peter good person                  |0       |\n",
            "|0    |22 |life Russia interesting            |0       |\n",
            "|0    |18 |John Peter brothers                |0       |\n",
            "|29   |53 |However dont support much          |1       |\n",
            "|0    |34 |Lucas Nogal Dunbercker longer happy|0       |\n",
            "|43   |57 |good car though                    |1       |\n",
            "|0    |18 |Europe culture rich                |0       |\n",
            "|29   |41 |huge churches                      |1       |\n",
            "|54   |63 |big houses                         |2       |\n",
            "+-----+---+-----------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we hadn't used Sentence Detector, this would be what we got. (tokenizer gets document instead of sentences column)\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "tokenassembler = TokenAssembler()\\\n",
        "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"clean_text\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler,\n",
        "                               tokenizer,\n",
        "                               normalizer,\n",
        "                               stopwords_cleaner,\n",
        "                               tokenassembler])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "result.select('text', 'clean_text.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szW09zVg6Q2_",
        "outputId": "60144e4e-cbad-4ea8-f490-9d19d760ea30"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|text                                                                         |result                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[Peter good person]                                  |\n",
            "|My life in Russia is very interesting.                                       |[life Russia interesting]                            |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[John Peter brothers However dont support much]      |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy good car though]|\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[Europe culture rich huge churches big houses]       |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.withColumn(\n",
        "    \"tmp\", \n",
        "    F.explode(\"clean_text\")) \\\n",
        "    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY2vc9tq6WYh",
        "outputId": "d53605ce-3e1d-4536-b3fc-9f78df4e9b37"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------------------------------------------------+--------+\n",
            "|begin|end|result                                             |sentence|\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "|0    |16 |Peter good person                                  |0       |\n",
            "|0    |22 |life Russia interesting                            |0       |\n",
            "|0    |44 |John Peter brothers However dont support much      |0       |\n",
            "|0    |50 |Lucas Nogal Dunbercker longer happy good car though|0       |\n",
            "|0    |43 |Europe culture rich huge churches big houses       |0       |\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemmer\n",
        "Returns hard-stems out of words with the objective of retrieving the meaningful part of the word"
      ],
      "metadata": {
        "id": "nyVRxwPM6axg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")"
      ],
      "metadata": {
        "id": "yZbtmWVQ6d0V"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               stemmer])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "\n",
        "result.show(truncate=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRMXAjUR6hPL",
        "outputId": "8d090bcd-23f6-417c-99c4-df8b7b6820d5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                                    text|                                document|                                   token|                                    stem|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|            Peter is a very good person.|[{document, 0, 27, Peter is a very go...|[{token, 0, 4, Peter, {sentence -> 0}...|[{token, 0, 4, peter, {sentence -> 0}...|\n",
            "|  My life in Russia is very interesting.|[{document, 0, 37, My life in Russia ...|[{token, 0, 1, My, {sentence -> 0}, [...|[{token, 0, 1, my, {sentence -> 0}, [...|\n",
            "|John and Peter are brothers. However ...|[{document, 0, 76, John and Peter are...|[{token, 0, 3, John, {sentence -> 0},...|[{token, 0, 3, john, {sentence -> 0},...|\n",
            "|Lucas Nogal Dunbercker is no longer h...|[{document, 0, 67, Lucas Nogal Dunber...|[{token, 0, 4, Lucas, {sentence -> 0}...|[{token, 0, 4, luca, {sentence -> 0},...|\n",
            "|Europe is very culture rich. There ar...|[{document, 0, 68, Europe is very cul...|[{token, 0, 5, Europe, {sentence -> 0...|[{token, 0, 5, europ, {sentence -> 0}...|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('stem.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbwWo0CX6kFA",
        "outputId": "6c0923aa-45b0-4c60-b39d-ad922f43d4bf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------+\n",
            "|result                                                                                     |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "|[peter, i, a, veri, good, person, .]                                                       |\n",
            "|[my, life, in, russia, i, veri, interest, .]                                               |\n",
            "|[john, and, peter, ar, brother, ., howev, thei, don't, support, each, other, that, much, .]|\n",
            "|[luca, nogal, dunberck, i, no, longer, happi, ., he, ha, a, good, car, though, .]          |\n",
            "|[europ, i, veri, cultur, rich, ., there, ar, huge, church, !, and, big, hous, !]           |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip(result.token.result, \n",
        "                                                 result.stem.result)).alias(\"cols\")) \\\n",
        "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "                          F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "XC9Mqhwk6sZQ",
        "outputId": "f88f8236-07a4-4f97-a547-d597ffc5b225"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    token    stem\n",
              "0   Peter   peter\n",
              "1      is       i\n",
              "2       a       a\n",
              "3    very    veri\n",
              "4    good    good\n",
              "5  person  person\n",
              "6       .       .\n",
              "7      My      my\n",
              "8    life    life\n",
              "9      in      in"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-867897c2-49c8-4ee5-958c-3203474b55bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter</td>\n",
              "      <td>peter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>very</td>\n",
              "      <td>veri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-867897c2-49c8-4ee5-958c-3203474b55bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-867897c2-49c8-4ee5-958c-3203474b55bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-867897c2-49c8-4ee5-958c-3203474b55bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizer\n",
        "Retrieves lemmas out of words with the objective of returning a base dictionary word"
      ],
      "metadata": {
        "id": "kih044a06xYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.addFile('https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt')"
      ],
      "metadata": {
        "id": "uvZWzrJj6wiy"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\") \\\n",
        "    .setDictionary(SparkFiles.get(\"AntBNC_lemmas_ver_001.txt\"), value_delimiter =\"\\t\", key_delimiter = \"->\")"
      ],
      "metadata": {
        "id": "6axl7CUb7Du3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ohkf8Dh7Kre",
        "outputId": "2e83f562-8590-4f3d-f0bf-5272d63f340a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='Lemmatizer_632e8ebfe975', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='Lemmatizer_632e8ebfe975', name='formCol', doc='Column that correspends to CoNLLU(formCol=) output'): 'form',\n",
              " Param(parent='Lemmatizer_632e8ebfe975', name='lemmaCol', doc='Column that correspends to CoNLLU(lemmaCol=) output'): 'lemma',\n",
              " Param(parent='Lemmatizer_632e8ebfe975', name='inputCols', doc='previous annotations columns, if renamed'): ['token'],\n",
              " Param(parent='Lemmatizer_632e8ebfe975', name='outputCol', doc='output annotation column. can be left default.'): 'lemma',\n",
              " Param(parent='Lemmatizer_632e8ebfe975', name='dictionary', doc=\"lemmatizer external dictionary. needs 'keyDelimiter' and 'valueDelimiter' in options for parsing target text\"): JavaObject id=o2752}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               stemmer,\n",
        "                               lemmatizer])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSVdve6F7NK4",
        "outputId": "1258752f-9b50-426d-b309-6712716149ab"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|               lemma|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, pe...|[{token, 0, 4, Pe...|\n",
            "|My life in Russia...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, my...|[{token, 0, 1, My...|\n",
            "|John and Peter ar...|[{document, 0, 76...|[{token, 0, 3, Jo...|[{token, 0, 3, jo...|[{token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[{document, 0, 68...|[{token, 0, 5, Eu...|[{token, 0, 5, eu...|[{token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('lemma.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O34Zne-f7SS8",
        "outputId": "e45b058b-30c7-476e-9b26-ae8e298d0bec"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------+\n",
            "|result                                                                                       |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "|[Peter, be, a, very, good, person, .]                                                        |\n",
            "|[My, life, in, Russia, be, very, interest, .]                                                |\n",
            "|[John, and, Peter, be, brother, ., However, they, don't, support, each, other, that, much, .]|\n",
            "|[Lucas, Nogal, Dunbercker, be, no, long, happy, ., He, have, a, good, car, though, .]        |\n",
            "|[Europe, be, very, culture, rich, ., There, be, huge, church, !, and, big, house, !]         |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip(result.token.result, \n",
        "                                                 result.stem.result, \n",
        "                                                 result.lemma.result)).alias(\"cols\")) \\\n",
        "                  .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "                          F.expr(\"cols['1']\").alias(\"stem\"),\n",
        "                          F.expr(\"cols['2']\").alias(\"lemma\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "13lmJBTg7Uze",
        "outputId": "f87d898c-287a-494b-ac39-907399daf001"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    token    stem   lemma\n",
              "0   Peter   peter   Peter\n",
              "1      is       i      be\n",
              "2       a       a       a\n",
              "3    very    veri    very\n",
              "4    good    good    good\n",
              "5  person  person  person\n",
              "6       .       .       .\n",
              "7      My      my      My\n",
              "8    life    life    life\n",
              "9      in      in      in"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-645c86fd-82e5-4f9f-9af5-46c3bfdcb2e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter</td>\n",
              "      <td>peter</td>\n",
              "      <td>Peter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>very</td>\n",
              "      <td>veri</td>\n",
              "      <td>very</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My</td>\n",
              "      <td>my</td>\n",
              "      <td>My</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-645c86fd-82e5-4f9f-9af5-46c3bfdcb2e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-645c86fd-82e5-4f9f-9af5-46c3bfdcb2e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-645c86fd-82e5-4f9f-9af5-46c3bfdcb2e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NGram Generator\n",
        "NGramGenerator annotator takes as input a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Stemmer, Lemmatizer, and StopWordsCleaner).\n",
        "\n",
        "The parameter n is used to determine the number of terms in each n-gram. The output will consist of a sequence of n-grams where each n-gram is represented by a space-delimited string of n consecutive words with annotatorType CHUNK same as the Chunker annotator.\n",
        "\n",
        "**Functions**:\n",
        "\n",
        "- setN: number elements per n-gram (>=1)\n",
        "- setEnableCumulative: whether to calculate just the actual n-grams or all n-grams from 1 through n\n",
        "- setDelimiter: Glue character used to join the tokens"
      ],
      "metadata": {
        "id": "NGtSOPej7cCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_cum = NGramGenerator() \\\n",
        "                .setInputCols([\"token\"]) \\\n",
        "                .setOutputCol(\"ngrams\") \\\n",
        "                .setN(3) \\\n",
        "                .setEnableCumulative(True)\\\n",
        "                .setDelimiter(\"_\") # Default is space\n",
        "    \n",
        "# .setN(3) means, take bigrams and trigrams.\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               ngrams_cum])\n",
        "\n",
        "result = nlpPipeline.fit(spark_df).transform(spark_df)\n",
        "result.select('ngrams.result').show(truncate=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5SskNnk7mCT",
        "outputId": "ed5c8048-7fe8-467f-b11f-27b2d01927bf"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                  result|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                    [Peter, is, a, very, good, person, ., Peter_is, is_a, a_very, very_good, good_person, person_., Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
            "|[My, life, in, Russia, is, very, interesting, ., My_life, life_in, in_Russia, Russia_is, is_very, very_interesting, interesting_., My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_...|\n",
            "|[John, and, Peter, are, brothers, ., However, they, don't, support, each, other, that, much, ., John_and, and_Peter, Peter_are, are_brothers, brothers_., ._However, However_they, they_don't, don't_...|\n",
            "|[Lucas, Nogal, Dunbercker, is, no, longer, happy, ., He, has, a, good, car, though, ., Lucas_Nogal, Nogal_Dunbercker, Dunbercker_is, is_no, no_longer, longer_happy, happy_., ._He, He_has, has_a, a_...|\n",
            "|[Europe, is, very, culture, rich, ., There, are, huge, churches, !, and, big, houses, !, Europe_is, is_very, very_culture, culture_rich, rich_., ._There, There_are, are_huge, huge_churches, churche...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams_nonCum = NGramGenerator() \\\n",
        "                  .setInputCols([\"token\"]) \\\n",
        "                  .setOutputCol(\"ngrams_v2\") \\\n",
        "                  .setN(3) \\\n",
        "                  .setEnableCumulative(False)\\\n",
        "                  .setDelimiter(\"_\") # Default is space\n",
        "    \n",
        "ngrams_nonCum.transform(result).select('ngrams_v2.result').show(truncate=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kcq7fd37wwL",
        "outputId": "bf022a9c-b64c-4100-c018-a614d9edaa94"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                  result|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                   [Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
            "|                                                                                                     [My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_interesting, very_interesting_.]|\n",
            "|[John_and_Peter, and_Peter_are, Peter_are_brothers, are_brothers_., brothers_._However, ._However_they, However_they_don't, they_don't_support, don't_support_each, support_each_other, each_other_th...|\n",
            "|   [Lucas_Nogal_Dunbercker, Nogal_Dunbercker_is, Dunbercker_is_no, is_no_longer, no_longer_happy, longer_happy_., happy_._He, ._He_has, He_has_a, has_a_good, a_good_car, good_car_though, car_though_.]|\n",
            "|[Europe_is_very, is_very_culture, very_culture_rich, culture_rich_., rich_._There, ._There_are, There_are_huge, are_huge_churches, huge_churches_!, churches_!_and, !_and_big, and_big_houses, big_ho...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextMatcher\n",
        "Annotator to match entire phrases (by token) provided in a file against a Document"
      ],
      "metadata": {
        "id": "AG3tW29V72P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"matched_entities\")\\\n",
        "\n",
        "entity_extractor.extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4MdQ68-71Bq",
        "outputId": "0a588178-d460-4c53-c18c-83288a72ca4e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='TextMatcher_4cd067a6e38f', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='TextMatcher_4cd067a6e38f', name='caseSensitive', doc='whether to match regardless of case. Defaults true'): True,\n",
              " Param(parent='TextMatcher_4cd067a6e38f', name='mergeOverlapping', doc='whether to merge overlapping matched chunks. Defaults false'): False,\n",
              " Param(parent='TextMatcher_4cd067a6e38f', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n",
              "  'token'],\n",
              " Param(parent='TextMatcher_4cd067a6e38f', name='outputCol', doc='output annotation column. can be left default.'): 'matched_entities'}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.addFile(\"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\")\n",
        "\n",
        "news_df = spark.read \\\n",
        "            .option(\"header\", True) \\\n",
        "            .csv(SparkFiles.get(\"news_category_train.csv\"))"
      ],
      "metadata": {
        "id": "wy-eJkQH7_mC"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dxRoKMt8PPa",
        "outputId": "96dc2e79-b5e1-4224-80f9-cd8d2bba52f5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business| Short sellers, Wall Street's dwindling band of...|\n",
            "|Business| Private investment firm Carlyle Group, which h...|\n",
            "|Business| Soaring crude prices plus worries about the ec...|\n",
            "|Business| Authorities have halted oil export flows from ...|\n",
            "|Business| Tearaway world oil prices, toppling records an...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = ['Wall Street', 'USD', 'stock', 'NYSE']\n",
        "with open ('financial_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')\n",
        "\n",
        "\n",
        "entities = ['soccer', 'world cup', 'Messi', 'FC Barcelona']\n",
        "with open ('sport_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')"
      ],
      "metadata": {
        "id": "aSeIfNn-8RZS"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "financial_entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"financial_entities\")\\\n",
        "    .setEntities(\"financial_entities.txt\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setEntityValue('financial_entity')\n",
        "\n",
        "sport_entity_extractor = TextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"sport_entities\")\\\n",
        "    .setEntities(\"sport_entities.txt\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setEntityValue('sport_entity')\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler, \n",
        "        tokenizer,\n",
        "        financial_entity_extractor,\n",
        "        sport_entity_extractor\n",
        "        ])\n",
        "\n",
        "result = nlpPipeline.fit(news_df).transform(news_df)"
      ],
      "metadata": {
        "id": "XYpW0YgM8KK6"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('financial_entities.result','sport_entities.result').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAWlROMN8mQb",
        "outputId": "f3c93009-8872-4c3a-820d-66b2e8320f26"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=[], result=[]),\n",
              " Row(result=[], result=[]),\n",
              " Row(result=['stock'], result=[])]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('description','financial_entities.result','sport_entities.result')\\\n",
        "      .toDF('text','financial_matches','sport_matches').filter((F.size('financial_matches')>1) | (F.size('sport_matches')>1))\\\n",
        "      .show(truncate=70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK0rtKO08rFT",
        "outputId": "729ca84f-2afb-4c73-d381-1acc3f27b136"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|                                                                  text|                 financial_matches|      sport_matches|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|\"Company launched the biggest electronic auction of stock in Wall S...|              [stock, Wall Street]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price for its ini...|                    [stock, stock]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price this mornin...|                    [stock, stock]|                 []|\n",
            "| Shares of Air Canada  (AC.TO) fell by more than half on Wednesday,...|                    [Stock, stock]|                 []|\n",
            "|Stock prices are lower in moderate trading. The Dow Jones Industria...|                    [Stock, Stock]|                 []|\n",
            "|The bad news just keeps pouring in for mutual fund manager Janus Ca...|                      [NYSE, NYSE]|                 []|\n",
            "|  Shaun Wright Phillips scored in his international debut as Englan...|                                []|[soccer, World Cup]|\n",
            "|NEWCASTLE, ENGLAND - England deservedly beat Ukraine 3-0 today in t...|                                []|[soccer, World Cup]|\n",
            "|MONTREAL (Reuters) - Shares of Air Canada (AC.TO: Quote, Profile, R...|                    [Stock, stock]|                 []|\n",
            "|\"SAN JOSE, California - On the cusp of its voyage into public tradi...|[stock, Wall Street, stock, Stock]|                 []|\n",
            "|\"Shortly before noon today, Google Inc. stock began trading under t...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: EA to take World Cup soccer to Xbox...IBM chalks up t...|                                []|[World Cup, soccer]|\n",
            "|The U.S. Securities and Exchange Commission yesterday approved Goog...|                    [stock, stock]|                 []|\n",
            "|After a bumpy ride toward becoming a publicly traded company, Googl...|                    [stock, stock]|                 []|\n",
            "|In the most highly anticipated Wall Street debut since the heady da...|              [Wall Street, stock]|                 []|\n",
            "|NEW YORK Despite voluble skepticism among investors, Google #39;s s...|                    [stock, stock]|                 []|\n",
            "|If only the rest of my investments worked out this way. One week ag...|                    [stock, stock]|                 []|\n",
            "| U.S. stocks to watch: GOOGLE INC. (GOOG.O) Google shares jumped 18...|                    [stock, stock]|                 []|\n",
            "|\" U.S. stocks to watch: GOOGLE INC.  &lt;A HREF=\"\"http://www.invest...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: KDE updates Linux desktop...EA to take World Cup socc...|                                []|[World Cup, soccer]|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip(result.financial_entities.result, \n",
        "                                                 result.financial_entities.begin, \n",
        "                                                 result.financial_entities.end)).alias(\"cols\")) \\\n",
        "                  .select(F.expr(\"cols['0']\").alias(\"clinical_entities\"),\n",
        "                          F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "                          F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "QrTtJVwd8yzs",
        "outputId": "40196983-d285-4bc0-e68b-7c8d350d7a36"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  clinical_entities  begin  end\n",
              "0             stock    112  116\n",
              "1             stock    114  118\n",
              "2             stock     45   49\n",
              "3             stock    126  130\n",
              "4             stock    188  192\n",
              "5             stock     52   56\n",
              "6       Wall Street     61   71\n",
              "7             stock     70   74\n",
              "8             stock    143  147\n",
              "9             stock    294  298"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dec86ed7-65e0-4626-9e59-09dc75d58fff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clinical_entities</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stock</td>\n",
              "      <td>112</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stock</td>\n",
              "      <td>114</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stock</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stock</td>\n",
              "      <td>126</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>stock</td>\n",
              "      <td>188</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>stock</td>\n",
              "      <td>52</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Wall Street</td>\n",
              "      <td>61</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>stock</td>\n",
              "      <td>70</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>stock</td>\n",
              "      <td>143</td>\n",
              "      <td>147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>stock</td>\n",
              "      <td>294</td>\n",
              "      <td>298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dec86ed7-65e0-4626-9e59-09dc75d58fff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dec86ed7-65e0-4626-9e59-09dc75d58fff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dec86ed7-65e0-4626-9e59-09dc75d58fff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexMatcher"
      ],
      "metadata": {
        "id": "LBgoyz1I88zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.addFile('https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv')\n",
        "pubMedDF = spark.read\\\n",
        "              .option(\"header\", \"true\")\\\n",
        "              .csv(SparkFiles.get(\"pubmed-sample.csv\"))\\\n",
        "              .filter(\"AB IS NOT null\")\\\n",
        "              .withColumnRenamed(\"AB\", \"text\")\\\n",
        "              .drop(\"TI\")\n",
        "\n",
        "pubMedDF.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDjOxThI9DOI",
        "outputId": "344ece43-462e-41ca-ba6e-1e4c367733c5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+\n",
            "|                                              text|\n",
            "+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|\n",
            "|BACKGROUND: At present, it is one of the most i...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|\n",
            "|Kohlschutter syndrome is a rare neurodegenerati...|\n",
            "|Statistical analysis of neuroimages is commonly...|\n",
            "|The synthetic DOX-LNA conjugate was characteriz...|\n",
            "|Our objective was to compare three different me...|\n",
            "|We conducted a phase II study to assess the eff...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|\n",
            "|We presented the tachinid fly Exorista japonica...|\n",
            "|The literature dealing with the water conductin...|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|\n",
            "|The localizing and lateralizing values of eye a...|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|\n",
            "|For the construction of new combinatorial libra...|\n",
            "|We report the results of a screen for genetic a...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|\n",
            "|It is known that patients with Klinefelter's sy...|\n",
            "+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules = '''\n",
        "renal\\s\\w+, started with 'renal'\n",
        "cardiac\\s\\w+, started with 'cardiac'\n",
        "\\w*ly\\b, ending with 'ly'\n",
        "\\S*\\d+\\S*, match any word that contains numbers\n",
        "(\\d+).?(\\d*)\\s*(mg|ml|g), match medication metrics\n",
        "'''\n",
        "\n",
        "with open('regex_rules.txt', 'w') as f:\n",
        "    \n",
        "    f.write(rules)\n",
        "    "
      ],
      "metadata": {
        "id": "hsUIqAwg9Wfl"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RegexMatcher().extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL1WSX8_9Zgo",
        "outputId": "f1478c4a-492c-4141-c1ec-63f290496863"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='RegexMatcher_a6addca18de4', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='RegexMatcher_a6addca18de4', name='strategy', doc='MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE'): 'MATCH_ALL'}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "regex_matcher = RegexMatcher()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setStrategy(\"MATCH_ALL\")\\\n",
        "    .setOutputCol(\"regex_matches\")\\\n",
        "    .setExternalRules(path='./regex_rules.txt', delimiter=',')\n",
        "    \n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler, \n",
        "        regex_matcher\n",
        "        ])\n",
        "\n",
        "match_df = nlpPipeline.fit(pubMedDF).transform(pubMedDF)\n",
        "match_df.select('regex_matches.result').take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A53bZaQB9dx0",
        "outputId": "2aad1559-29ea-4ef4-83ab-a80552c8987e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['inwardly', 'family', 'spansapproximately', 'byapproximately', 'approximately', 'respectively', 'poly', 'KCNJ9', '3.3,', 'GIRK3)', 'KCNJ9', '1q21-23', '7.6', '2.2', '2.6', 'identified14', 'aVal366Ala', '8', 'KCNJ9', 'KCNJ9', '9 g']),\n",
              " Row(result=['previously', 'previously', 'intravenously', 'previously', '25', 'mg/m(2)', '1', '8', 'a3', '50', '20.0%', '(10', '50;', '95%', 'interval,10.0-33.7%).', '58.0%', '[10', '18', '50].', '(50%', '115.0', '17.3%', '52).', '25 mg']),\n",
              " Row(result=['renal failure', 'cardiac surgery', 'cardiac surgery', 'cardiac surgical', 'early', 'statistically', 'analy', '1995', '2005', '=9796).', '2.9', '11years).', '11.3%', '1105),', '7.2%', '30%', '0.0001),', '1.55,95%', '1.42-1.70,', '0.0001).'])]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.select('text','regex_matches.result')\\\n",
        "        .toDF('text','matches').filter(F.size('matches')>1)\\\n",
        "        .show(truncate=70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBItP8Cp9i0e",
        "outputId": "6d322384-e862-4e0b-bcdd-42afda307bbe"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|                                                                  text|                                                               matches|\n",
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activ...|[inwardly, family, spansapproximately, byapproximately, approximate...|\n",
            "|BACKGROUND: At present, it is one of the most important issues for ...|[previously, previously, intravenously, previously, 25, mg/m(2), 1,...|\n",
            "|OBJECTIVE: To investigate the relationship between preoperative atr...|[renal failure, cardiac surgery, cardiac surgery, cardiac surgical,...|\n",
            "|Combined EEG/fMRI recording has been used to localize the generator...|[normally, significantly, effectively, analy, only, considerably, 2...|\n",
            "|Statistical analysis of neuroimages is commonly approached with int...|[analy, commonly, overly, normally, thatsuccessfully, recently, ana...|\n",
            "|The synthetic DOX-LNA conjugate was characterized by proton nuclear...|                                             [wasanaly, substantially]|\n",
            "|Our objective was to compare three different methods of blood press...|[daily, only, Conversely, Hourly, hourly, Hourly, hourly, hourly, h...|\n",
            "|We conducted a phase II study to assess the efficacy and tolerabili...|[analy, respectively, generally, 5-fluorouracil, (5-FU)-, 5-FU-base...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoenzyme that catalyzes...|[cataly, methylgly, gly, ethylgly, dimethylgly, spectrally, practic...|\n",
            "|We presented the tachinid fly Exorista japonica with moving host mo...|                                             [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conducting properties of sapw...|                               [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopropyl-5'-O-d4T monoph...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d4T, Chitosan-d4T, 1....|\n",
            "|An HPLC-ESI-MS-MS method has been developed for the quantitative de...|[chromatographically, respectively, successfully, C18, (n=5), 95.0%...|\n",
            "|The localizing and lateralizing values of eye and head ictal deviat...|                                                        [early, early]|\n",
            "|OBJECTIVE: To evaluate the effectiveness and acceptability of expec...|[weekly, respectively, theanaly, 2006, 2007,, 2, 66, 1), 30patients...|\n",
            "|We report the results of a screen for genetic association with urin...|[poly, threepoly, significantly, analy, actually, anextremely, only...|\n",
            "|Intraparenchymal pericatheter cyst is rarely reported. Obstruction ...|                                  [rarely, possibly, unusually, Early]|\n",
            "|PURPOSE: To compare the effectiveness, potential advantages and com...|[analy, comparatively, wassignificantly, respectively, a7-year, 155...|\n",
            "|We have demonstrated a new type of all-optical 2 x 2 switch by usin...|[approximately, fully, approximately, approximately, approximately,...|\n",
            "|Physalis peruviana (PP) is a widely used medicinal herb for treatin...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82.3%), onFeCl2-ascorb...|\n",
            "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiDateMatcher\n",
        "Extract exact & normalize dates from relative date-time phrases. The default anchor date will be the date the code is run.\n",
        "Extract exact & normalize dates from relative date-time phrases. The default anchor date will be the date the code is run"
      ],
      "metadata": {
        "id": "yp9871w99sEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MultiDateMatcher().extractParamMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF-swzy29wqM",
        "outputId": "4c4de3fa-a9d6-4d19-9bdd-cb991a3b887b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Param(parent='MultiDateMatcher_de99da0f3633', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n",
              " Param(parent='MultiDateMatcher_de99da0f3633', name='inputFormats', doc='input formats list of patterns to match'): [''],\n",
              " Param(parent='MultiDateMatcher_de99da0f3633', name='outputFormat', doc='desired output format for dates extracted'): 'yyyy/MM/dd',\n",
              " Param(parent='MultiDateMatcher_de99da0f3633', name='readMonthFirst', doc='Whether to parse july 07/05/2015 or as 05/07/2015'): True,\n",
              " Param(parent='MultiDateMatcher_de99da0f3633', name='defaultDayWhenMissing', doc='which day to set when it is missing from parsed input'): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "date_matcher = MultiDateMatcher() \\\n",
        "    .setInputCols('document') \\\n",
        "    .setOutputCol(\"date\")\\\n",
        "    .setOutputFormat(\"yyyy/MM/dd\")\\\n",
        "    .setSourceLanguage(\"en\")\n",
        "\n",
        "\n",
        "date_pipeline = PipelineModel(\n",
        "    stages=[\n",
        "        documentAssembler, \n",
        "        date_matcher\n",
        "        ])\n",
        "\n",
        "sample_df = spark.createDataFrame([['I saw him yesterday and he told me that he will visit us next week']]).toDF(\"text\")\n",
        "\n",
        "result = date_pipeline.transform(sample_df)\n",
        "result.select('date.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFb3LpWQ9z8_",
        "outputId": "86fcf88b-5c9d-43b3-82b0-c8899d74643b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+\n",
            "|result                  |\n",
            "+------------------------+\n",
            "|[2022/12/29, 2022/12/21]|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "date_matcher = MultiDateMatcher() \\\n",
        "    .setInputCols('document') \\\n",
        "    .setOutputCol(\"date\")\\\n",
        "    .setInputFormats([\"dd/MM/yyyy\"])\\\n",
        "    .setOutputFormat(\"yyyy/MM/dd\")\\\n",
        "    .setSourceLanguage(\"en\")\n",
        "\n",
        "date_pipeline = PipelineModel(\n",
        "    stages=[\n",
        "        documentAssembler, \n",
        "        date_matcher\n",
        "        ])\n",
        "\n",
        "sample_df = spark.createDataFrame([[\"the last payment date of this invoice is 21/05/2022\"]]).toDF(\"text\")\n",
        "\n",
        "result = date_pipeline.transform(sample_df)\n",
        "result.select('date.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZV6kEBh-EeB",
        "outputId": "625b60b5-34b7-4089-caf3-b63fd5d0260e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|result      |\n",
            "+------------+\n",
            "|[2022/05/21]|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning\n"
      ],
      "metadata": {
        "id": "Iszt1SJb-QP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
        "\n",
        "text_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
        "\n",
        "import re\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "clean_text = lambda s: re.sub(r'<[^>]*>', '', s)\n",
        "\n",
        "text_df.withColumn('cleaned', udf(clean_text, StringType())('text')).select('text','cleaned').show(truncate= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHdIYtXr-SQ0",
        "outputId": "a0fced14-9176-4d91-cd53-7cc3d2d2c536"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "|text                                                                                          |cleaned                |\n",
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|Have a great birth day!|\n",
            "+----------------------------------------------------------------------------------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_not_alnum_count = lambda s: len([i for i in s if not i.isalnum() and i!=' '])\n",
        "\n",
        "find_not_alnum_count(\"it's your birth day!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWbqeuNh-jKz",
        "outputId": "37dba114-9d53-450e-8bbc-f5a327fdb4e4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n",
        "\n",
        "find_not_alnum_count(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl-VtNyO-n5x",
        "outputId": "79de08d7-e43e-4062-99b6-f0e5ccb74a99"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_df.withColumn('cleaned', udf(find_not_alnum_count, IntegerType())('text')).select('text','cleaned').show(truncate= False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBAqHigp-q7g",
        "outputId": "ee7d1114-e5d8-4833-db00-cd89e21cce08"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "|text                                                                                          |cleaned|\n",
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "|<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>|23     |\n",
            "+----------------------------------------------------------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finisher¶\n",
        "Finisher: Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n",
        "\n",
        "If we just want the desired output column in the final dataframe, we can use Finisher to drop previous stages in the final output and get the result from the process.\n",
        "\n",
        "This is very handy when you want to use the output from Spark NLP annotator as an input to another Spark ML transformer."
      ],
      "metadata": {
        "id": "jRLj3bd7-wRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"regex_matches\"]) \\\n",
        "    .setIncludeMetadata(False) # set to False to remove metadata\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               regex_matcher,\n",
        "                               finisher])\n",
        "\n",
        "match_df = nlpPipeline.fit(pubMedDF).transform(pubMedDF)\n",
        "match_df.show(truncate = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek1Y2TEZ-0eo",
        "outputId": "d32d4d73-3859-404f-bfe4-f6c9b80fc24b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
            "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
            "|Kohlschutter syndrome is a rare neurodegenerati...|                                          [family]|\n",
            "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
            "|The synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\n",
            "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
            "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
            "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
            "|The localizing and lateralizing values of eye a...|                                    [early, early]|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
            "|For the construction of new combinatorial libra...|                                           [newly]|\n",
            "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
            "|It is known that patients with Klinefelter's sy...|                                                []|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47IsjmyA-9Xr",
        "outputId": "af530410-fbe3-41c5-9351-b2b0b7671b2e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- finished_regex_matches: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_df.filter(F.size('finished_regex_matches')>2).show(truncate = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cas6cU4_A7U",
        "outputId": "e8ad9c5c-5e63-4b8e-fbe9-023a17d7d277"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\n",
            "|BACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\n",
            "|Statistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\n",
            "|Our objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\n",
            "|We conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\n",
            "|We presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\n",
            "|The literature dealing with the water conductin...|           [generally, mathematically, especially]|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5'-O-d...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\n",
            "|We report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\n",
            "|PURPOSE: To compare the effectiveness, potentia...|[analy, comparatively, wassignificantly, respec...|\n",
            "|We have demonstrated a new type of all-optical ...|[approximately, fully, approximately, approxima...|\n",
            "|Physalis peruviana (PP) is a widely used medici...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82...|\n",
            "|We report the discovery of a series of substitu...|[highly, potentially, highly, respectively, tub...|\n",
            "|The purpose of this study was to identify and c...|[family, Nearly, only, 43, 10, 44%, 32%, 64%, 4...|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightPipeline¶\n",
        "https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1\n",
        "\n",
        "LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n",
        "\n",
        "Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n",
        "\n",
        "It is nearly 10x faster than using Spark ML Pipeline\n",
        "\n",
        "LightPipeline(someTrainedPipeline).annotate(someStringOrArray)"
      ],
      "metadata": {
        "id": "PX2nfeNB_KLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler, \n",
        "        tokenizer,\n",
        "        stemmer,\n",
        "        lemmatizer\n",
        "        ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "nlpPipeline.fit(empty_df).transform(spark_df).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJtT6COx_H00",
        "outputId": "2b394000-149e-4b73-dba1-f6d3c9999289"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|               lemma|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[{document, 0, 27...|[{token, 0, 4, Pe...|[{token, 0, 4, pe...|[{token, 0, 4, Pe...|\n",
            "|My life in Russia...|[{document, 0, 37...|[{token, 0, 1, My...|[{token, 0, 1, my...|[{token, 0, 1, My...|\n",
            "|John and Peter ar...|[{document, 0, 76...|[{token, 0, 3, Jo...|[{token, 0, 3, jo...|[{token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[{document, 0, 67...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[{document, 0, 68...|[{token, 0, 5, Eu...|[{token, 0, 5, eu...|[{token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "light_model = LightPipeline(pipelineModel)\n",
        "light_result = light_model.annotate(\"John and Peter are brothers. However they don't support each other that much.\")"
      ],
      "metadata": {
        "id": "Xxv1WHLj_UIb"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_result.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7naBu1G-_WWN",
        "outputId": "e26f7bfe-348d-47e2-ba39-1d93d924b840"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'token', 'stem', 'lemma'])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(light_result['token'], light_result['stem'], light_result['lemma']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8lz-vI_ZFj",
        "outputId": "d2f7082e-ef4e-4179-a242-c6607e2ab764"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'john', 'John'),\n",
              " ('and', 'and', 'and'),\n",
              " ('Peter', 'peter', 'Peter'),\n",
              " ('are', 'ar', 'be'),\n",
              " ('brothers', 'brother', 'brother'),\n",
              " ('.', '.', '.'),\n",
              " ('However', 'howev', 'However'),\n",
              " ('they', 'thei', 'they'),\n",
              " (\"don't\", \"don't\", \"don't\"),\n",
              " ('support', 'support', 'support'),\n",
              " ('each', 'each', 'each'),\n",
              " ('other', 'other', 'other'),\n",
              " ('that', 'that', 'that'),\n",
              " ('much', 'much', 'much'),\n",
              " ('.', '.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "light_result = light_model.fullAnnotate(\"John and Peter are brothers. However they don't support each other that much.\")"
      ],
      "metadata": {
        "id": "060tuMNR_b1l"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxRfoRe2_eSw",
        "outputId": "62fc2eca-40d6-4362-a0bb-1debda702b72"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': [Annotation(document, 0, 76, John and Peter are brothers. However they don't support each other that much., {})],\n",
              "  'token': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, are, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brothers, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
              "  'stem': [Annotation(token, 0, 3, john, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, ar, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, howev, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, thei, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})],\n",
              "  'lemma': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, be, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'}),\n",
              "   Annotation(token, 76, 76, ., {'sentence': '0'})]}]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list= [\"How did serfdom develop in and then leave Russia ?\",\n",
        "\"There will be some exciting breakthroughs in NLP this year.\"]\n",
        "\n",
        "light_model.annotate(text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jWGb8P__gxU",
        "outputId": "879f98ed-fcdd-46af-9f83-c177ffa17ddb"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': ['How did serfdom develop in and then leave Russia ?'],\n",
              "  'token': ['How',\n",
              "   'did',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leave',\n",
              "   'Russia',\n",
              "   '?'],\n",
              "  'stem': ['how',\n",
              "   'did',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leav',\n",
              "   'russia',\n",
              "   '?'],\n",
              "  'lemma': ['How',\n",
              "   'do',\n",
              "   'serfdom',\n",
              "   'develop',\n",
              "   'in',\n",
              "   'and',\n",
              "   'then',\n",
              "   'leave',\n",
              "   'Russia',\n",
              "   '?']},\n",
              " {'document': ['There will be some exciting breakthroughs in NLP this year.'],\n",
              "  'token': ['There',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'exciting',\n",
              "   'breakthroughs',\n",
              "   'in',\n",
              "   'NLP',\n",
              "   'this',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'stem': ['there',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'excit',\n",
              "   'breakthrough',\n",
              "   'in',\n",
              "   'nlp',\n",
              "   'thi',\n",
              "   'year',\n",
              "   '.'],\n",
              "  'lemma': ['There',\n",
              "   'will',\n",
              "   'be',\n",
              "   'some',\n",
              "   'exciting',\n",
              "   'breakthrough',\n",
              "   'in',\n",
              "   'NLP',\n",
              "   'this',\n",
              "   'year',\n",
              "   '.']}]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    }
  ]
}
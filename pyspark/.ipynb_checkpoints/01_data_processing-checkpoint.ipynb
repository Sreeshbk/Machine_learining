{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b12f1b-6aee-44ba-a2db-d554c7c310a9",
   "metadata": {},
   "source": [
    "# Data Processing using pyspark\n",
    "\n",
    "This Notebook covers the algorithms for\n",
    "\n",
    "- Extraction: Extracting features from “raw” data\n",
    "- Transformation: Scaling, converting, or modifying features\n",
    "- Selection: Selecting a subset from a larger set of features\n",
    "- Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03bb63c-c427-44cc-9367-87ed2c006947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('Dataprocessing using pyspark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4887abd-f9aa-453b-8cff-9e613b52de0b",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9dd86-8aed-476d-afa8-d613891ba77a",
   "metadata": {},
   "source": [
    "### 1.1 TF- IDF [Term Frequency - Inverse Document Frequency ] \n",
    "Feature Vectorization Technique method used for data mining to reflect the importance of term to a document in the corpus.\n",
    "\n",
    "Denote a term by `t`, a document by `d`, and the corpus by `D`. Term frequency `TF(t,d)` is the number of times that term `t` appears in document `d`, while document frequency `DF(t,D)` is the number of documents that contains term `t`.\n",
    "\n",
    "\\begin{equation}\n",
    "    IDF(t,d) = log \\frac{|D|+1}{DF(t,D)+1}\n",
    "\\end{equation}\n",
    "\n",
    "where `|D|` is the total number of documents in the corpus\n",
    "\n",
    "\\begin{equation}\n",
    "TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fcc0c-59c9-4f47-bce0-1b415011a640",
   "metadata": {},
   "source": [
    "**TF** : HashingTF and CountVectorizer can be used for Term Frequency\n",
    "\n",
    "`HashingTF` is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors.\n",
    "HashingTF utilizes the hashing trick. A raw feature is mapped into an index (term) by applying a hash function. The hash function used here is MurmurHash 3.\n",
    "\n",
    "`CountVectorizer` converts text documents to vectors of term counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7142191f-128e-442e-a107-5858e6b6f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+\n",
      "|label|sentence                           |\n",
      "+-----+-----------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |\n",
      "|0.0  |I wish Java could use case classes |\n",
      "|1.0  |Logistic regression models are neat|\n",
      "+-----+-----------------------------------+\n",
      "\n",
      "Tokenize the sentence \n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|label|sentence                           |words                                     |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "\n",
      "Transform to TF - HashingTF \n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------+\n",
      "|label|sentence                           |words                                     |rawFeatures                                    |\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |(20,[6,8,13,16],[1.0,1.0,1.0,2.0])             |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(20,[0,2,7,13,15,16],[1.0,1.0,2.0,1.0,1.0,1.0])|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(20,[3,4,6,11,19],[1.0,1.0,1.0,1.0,1.0])       |\n",
      "+-----+-----------------------------------+------------------------------------------+-----------------------------------------------+\n",
      "\n",
      "Transform to TF - CountVectorizer \n",
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "|label|sentence                           |words                                     |rawFeatures1                                          |\n",
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |(16,[0,3,7,8,9],[1.0,1.0,1.0,1.0,1.0])                |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(16,[0,4,5,10,11,13,15],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(16,[1,2,6,12,14],[1.0,1.0,1.0,1.0,1.0])              |\n",
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "sentenceData.show(truncate=False)\n",
    "\n",
    "print(\"Tokenize the sentence \")\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "word_data = tokenizer.transform(sentenceData)\n",
    "word_data.show(truncate=False)\n",
    "\n",
    "print(\"Transform to TF - HashingTF \")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurized_data = hashingTF.transform(word_data)\n",
    "featurized_data.show(truncate=False)\n",
    "\n",
    "print(\"Transform to TF - CountVectorizer \")\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures1\")\n",
    "c_featurized_data = cv.fit(word_data).transform(word_data)\n",
    "c_featurized_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb494d7a-2b9b-4f35-a423-5c28a0105146",
   "metadata": {},
   "source": [
    "**IDF** : IDF is an Estimator which is fit on a dataset and produces an `IDFModel`. The `IDFModel` takes feature vectors (generally created from `HashingTF` or `CountVectorizer`) and scales each feature. Intuitively, it down-weights features which appear frequently in a corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605b8083-5521-422c-adc5-0418cab15b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                   |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(20,[6,8,13,16],[0.28768207245178085,0.6931471805599453,0.28768207245178085,0.5753641449035617])                                           |\n",
      "|0.0  |(20,[0,2,7,13,15,16],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.28768207245178085,0.6931471805599453,0.28768207245178085])|\n",
      "|1.0  |(20,[3,4,6,11,19],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])                       |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaledData = idfModel.transform(featurized_data)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef7f85-a99d-4833-b17d-de91a2ef4664",
   "metadata": {},
   "source": [
    "## 1.2 Word2Vec\n",
    "\n",
    "`Word2Vec` is an Estimator which takes sequences of words representing documents and trains a `Word2VecModel`. \n",
    "- The model maps each word to a unique fixed-size vector. \n",
    "- The `Word2VecModel` transforms each document into a vector using the average of all words in the document; \n",
    "- This vector can then be used as features for prediction, document similarity calculations, etc.\n",
    "- Computes distributed vector representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd7f96a-6280-4c7a-b8fa-a2194b23db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.024519025813788176,-0.025680204108357432,-0.010560354497283698]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.0755529555359057,0.04371664673089981,-0.08695207749094282]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.03579759057611227,0.012968631088733674,-0.0072596315294504166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e328d41b-18c7-4a11-b593-91edcd85a419",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m synonyms \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfindSynonyms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, cosine_distance \u001b[38;5;129;01min\u001b[39;00m synonyms:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(word, cosine_distance))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:560\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d20026-6f91-4bd4-b044-3d6c4403022d",
   "metadata": {},
   "source": [
    "# Load data for Machine Learning and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d8dbd-191f-4cf5-bc43-8adc2f5fcdcb",
   "metadata": {},
   "source": [
    "This Notebook covers information about loading data specifically for ML and DL application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcadef-a1f1-4e21-ab8a-dd49cb7a0ce6",
   "metadata": {},
   "source": [
    "## Petastorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a76ec-fbf9-41fb-8659-9e5e48e8e000",
   "metadata": {},
   "source": [
    "Petastorm is an opensource data access library that enables directly loading data stored in Apache Parquet Format. This library enables single-node or distributed training and evaluation of deep learning model directly from datasets in Apache parquet format and dataset that are loaded as Apache Spark Data frame\n",
    "\n",
    "Petastorm is an open source data access library developed at **Uber ATG**. This library enables single machine or distributed training and evaluation of deep learning models directly from datasets in Apache Parquet format. Petastorm supports popular Python-based machine learning (ML) frameworks such as Tensorflow, PyTorch, and PySpark. It can also be used from pure Python code.\n",
    "\n",
    "**Installation**\n",
    "```\n",
    "pip install petastorm\n",
    "```\n",
    "There are several extra dependencies that are defined by the petastorm package that are not installed automatically. The extras are: tf, tf_gpu, torch, opencv, docs, test.\n",
    "\n",
    "For example to trigger installation of GPU version of tensorflow and opencv, use the following pip command:\n",
    "```\n",
    "pip install petastorm[opencv,tf_gpu]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20178c79-2124-48ba-abd1-d174aded7859",
   "metadata": {},
   "source": [
    "Petastorm Spark convertor API simplifies data conversion from Spark to Tensorflow or Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97066ff-882a-4681-be34-86bc122311a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('Loading Data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f90ed37-85cf-4167-bd48-ac84b908fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n",
      "/tmp/ipykernel_31178/2371677732.py:2: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  from pyarrow import LocalFileSystem\n"
     ]
    }
   ],
   "source": [
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'file:///home/jovyan/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0d40e7-6b80-4868-a988-770b2eed0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bdcc63-f934-46f4-91a2-785824e5e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema defines how the dataset schema looks like\n",
    "HelloWorldSchema = Unischema('HelloWorldSchema', [\n",
    "    UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\n",
    "    UnischemaField('image1', np.uint8, (128, 256, 3), CompressedImageCodec('png'), False),\n",
    "    UnischemaField('array_4d', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8ba9d-58a8-4223-b57f-775cfb969fb1",
   "metadata": {},
   "source": [
    "`HelloWorldSchema` is an instance of a `Unischema` object. `Unischema` is capable of rendering types of its fields into different framework specific formats, such as: Spark StructType, Tensorflow tf.DType and numpy numpy.dtype.\n",
    "\n",
    "To define a dataset field, you need to specify a type, shape, a codec instance and whether the field is nullable for each field of the Unischema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c37e20-f62e-46b3-9dee-84bef438904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_generator(x):\n",
    "    \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\n",
    "    return {'id': x,\n",
    "            'image1': np.random.randint(0, 255, dtype=np.uint8, size=(128, 256, 3)),\n",
    "            'array_4d': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdff59c-f61e-4ac8-90ed-80e00531370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_petastorm_dataset(output_url='file:///tmp/hello_world_dataset'):\n",
    "    rowgroup_size_mb = 256\n",
    "\n",
    "    spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\n",
    "    # well as save petastorm specific metadata\n",
    "    rows_count = 10\n",
    "    with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\n",
    "\n",
    "        rows_rdd = sc.parallelize(range(rows_count))\\\n",
    "            .map(row_generator)\\\n",
    "            .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\n",
    "\n",
    "        spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\n",
    "            .coalesce(10) \\\n",
    "            .write \\\n",
    "            .mode('overwrite') \\\n",
    "            .parquet(output_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f689d-1dc0-492b-ab4b-c3579e6b6111",
   "metadata": {},
   "source": [
    "- We wrap spark dataset generation code with the `materialize_dataset` context manager. The context manager is responsible for configuring row group size at the beginning and write out petastorm specific metadata at the end.\n",
    "- The row generating code is expected to return a Python dictionary indexed by a field name. We use `row_generator` function for that.\n",
    "- `dict_to_spark_row converts` the dictionary into a pyspark.Row object while ensuring schema HelloWorldSchema compliance (shape, type and is-nullable condition are tested).\n",
    "- Once we have a pyspark.DataFrame we write it out to a parquet storage. The parquet schema is automatically derived from HelloWorldSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5711ee41-b839-4504-8a1d-f755f1c4c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/petastorm/fs_utils.py:89: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem_factory = lambda: pyarrow.localfs\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:104: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  if dataset.fs.exists(common_metadata_file_path):\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:107: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  elif dataset.fs.exists(metadata_file_path):\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:113: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  arrow_metadata = dataset.pieces[0].get_metadata()\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:122: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  with dataset.fs.open(common_metadata_file_path, 'wb') as metadata_file:\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:128: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  if isinstance(dataset.fs, LocalFileSystem) and dataset.fs.exists(common_metadata_file_crc_path):\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:223: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  paths = [piece.path for piece in dataset.pieces]\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/utils.py:105: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  with dataset.fs.open(common_metadata_file_path) as f:\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  metadata = dataset.metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  common_metadata = dataset.common_metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:278: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  sorted_pieces = sorted(dataset.pieces, key=attrgetter('path'))\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n"
     ]
    }
   ],
   "source": [
    "generate_petastorm_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f903d56-ed4a-4be7-ab2f-245c1f264559",
   "metadata": {},
   "source": [
    "## Plain Python API\n",
    "The petastorm.reader.Reader class is the main entry point for user code that accesses the data from an ML framework such as Tensorflow or Pytorch. The reader has multiple features such as:\n",
    "\n",
    "- Selective column readout\n",
    "- Multiple parallelism strategies: thread, process, single-threaded (for debug)\n",
    "- N-grams readout support\n",
    "- Row filtering (row predicates)\n",
    "- Shuffling\n",
    "- Partitioning for multi-GPU training\n",
    "- Local caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2091cd4e-f52f-43bf-bad8-50e51b5dcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorldSchema_view(id=2, image1=array([[[ 99,  56, 115],\n",
      "        [141,  88,  32],\n",
      "        [253,   6,  80],\n",
      "        ...,\n",
      "        [133, 248,  32],\n",
      "        [130, 216, 171],\n",
      "        [162,  70, 249]],\n",
      "\n",
      "       [[200,  10, 104],\n",
      "        [ 72,  10, 196],\n",
      "        [ 11,  10, 185],\n",
      "        ...,\n",
      "        [ 96, 131, 158],\n",
      "        [150,  31, 178],\n",
      "        [106,  59,  55]],\n",
      "\n",
      "       [[ 23, 110, 254],\n",
      "        [144, 254, 203],\n",
      "        [104, 108,  64],\n",
      "        ...,\n",
      "        [150, 168, 200],\n",
      "        [102, 102, 188],\n",
      "        [184, 169, 251]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[157, 226, 161],\n",
      "        [ 19, 204, 107],\n",
      "        [171,  81, 208],\n",
      "        ...,\n",
      "        [199, 114, 198],\n",
      "        [223,  82, 151],\n",
      "        [209, 151, 127]],\n",
      "\n",
      "       [[206,  90, 254],\n",
      "        [180, 253,  91],\n",
      "        [ 13,  69, 128],\n",
      "        ...,\n",
      "        [ 96, 212, 237],\n",
      "        [ 44, 224, 224],\n",
      "        [238,  59, 235]],\n",
      "\n",
      "       [[241,  18, 154],\n",
      "        [ 17, 223, 235],\n",
      "        [221, 111, 168],\n",
      "        ...,\n",
      "        [ 32,  67,  27],\n",
      "        [167,  30,  28],\n",
      "        [186, 243, 127]]], dtype=uint8), array_4d=array([[[[128,  50, 138],\n",
      "         [109, 189,  77],\n",
      "         [194, 139, 136],\n",
      "         ...,\n",
      "         [199,  59, 138],\n",
      "         [239, 198, 166],\n",
      "         [ 97,   3, 107]],\n",
      "\n",
      "        [[ 34, 140,  88],\n",
      "         [ 64, 186,  83],\n",
      "         [ 26, 241,  81],\n",
      "         ...,\n",
      "         [  1, 168, 238],\n",
      "         [188, 192,  33],\n",
      "         [ 45, 222,  56]],\n",
      "\n",
      "        [[200,  68, 175],\n",
      "         [147, 142,  54],\n",
      "         [221, 153, 245],\n",
      "         ...,\n",
      "         [217,   7,  72],\n",
      "         [164, 103, 223],\n",
      "         [170,  72,  68]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[133, 127, 166],\n",
      "         [224,  98,  82],\n",
      "         [107, 136,  70],\n",
      "         ...,\n",
      "         [167, 226, 147],\n",
      "         [ 60,  58, 103],\n",
      "         [127, 199,   6]],\n",
      "\n",
      "        [[  7, 185,   7],\n",
      "         [192,  48, 168],\n",
      "         [252,   0, 196],\n",
      "         ...,\n",
      "         [190,  67, 247],\n",
      "         [185, 125, 239],\n",
      "         [224, 131, 108]],\n",
      "\n",
      "        [[222, 121,   4],\n",
      "         [ 92,  71,  78],\n",
      "         [192, 113, 127],\n",
      "         ...,\n",
      "         [233,  95, 196],\n",
      "         [116, 210, 133],\n",
      "         [209,  72, 105]]],\n",
      "\n",
      "\n",
      "       [[[167, 186, 202],\n",
      "         [229, 156, 166],\n",
      "         [ 95, 244, 233],\n",
      "         ...,\n",
      "         [ 45,  94,  91],\n",
      "         [169,  47, 150],\n",
      "         [  6,  37, 112]],\n",
      "\n",
      "        [[225, 212,  55],\n",
      "         [ 40,  37, 123],\n",
      "         [ 53,  44, 160],\n",
      "         ...,\n",
      "         [ 49,  99, 228],\n",
      "         [201, 243, 137],\n",
      "         [  8, 147,  88]],\n",
      "\n",
      "        [[196,  72,  13],\n",
      "         [207, 223,  77],\n",
      "         [ 82,  52, 195],\n",
      "         ...,\n",
      "         [233, 165, 141],\n",
      "         [ 84, 128, 171],\n",
      "         [ 91, 250, 176]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[242, 109,  79],\n",
      "         [ 17,  21,  69],\n",
      "         [ 38, 231, 135],\n",
      "         ...,\n",
      "         [188, 243, 140],\n",
      "         [ 47, 219, 182],\n",
      "         [172,  84, 153]],\n",
      "\n",
      "        [[232,  55,  74],\n",
      "         [ 72,  25,  59],\n",
      "         [ 69, 131,  68],\n",
      "         ...,\n",
      "         [219, 198,  20],\n",
      "         [ 98,  14, 244],\n",
      "         [150, 236, 130]],\n",
      "\n",
      "        [[ 65,  26, 159],\n",
      "         [133,  93, 139],\n",
      "         [199, 224, 204],\n",
      "         ...,\n",
      "         [ 54, 201, 145],\n",
      "         [165,  44, 168],\n",
      "         [217, 223, 144]]],\n",
      "\n",
      "\n",
      "       [[[ 29,  31,  53],\n",
      "         [217, 240, 164],\n",
      "         [ 18, 155,  21],\n",
      "         ...,\n",
      "         [215, 220, 233],\n",
      "         [ 72, 177,  90],\n",
      "         [106, 112,   2]],\n",
      "\n",
      "        [[164, 140, 178],\n",
      "         [236,  56,   9],\n",
      "         [ 27, 100, 106],\n",
      "         ...,\n",
      "         [170,  60, 188],\n",
      "         [ 72,   9,  39],\n",
      "         [233, 111,  11]],\n",
      "\n",
      "        [[208, 143, 160],\n",
      "         [ 51,  61,  24],\n",
      "         [147,  76, 216],\n",
      "         ...,\n",
      "         [253,  71, 169],\n",
      "         [169,  32, 167],\n",
      "         [142, 251, 205]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[249, 163, 236],\n",
      "         [192,   3, 229],\n",
      "         [ 51,  11, 138],\n",
      "         ...,\n",
      "         [ 60, 117,  43],\n",
      "         [103, 163, 149],\n",
      "         [208, 225,  84]],\n",
      "\n",
      "        [[201, 253, 154],\n",
      "         [127, 204,  67],\n",
      "         [ 54, 248, 104],\n",
      "         ...,\n",
      "         [253, 194, 217],\n",
      "         [158,  95, 225],\n",
      "         [ 83,  15, 204]],\n",
      "\n",
      "        [[125,   8, 237],\n",
      "         [ 69,  54, 143],\n",
      "         [205, 225,  12],\n",
      "         ...,\n",
      "         [ 61,   3,  10],\n",
      "         [100,  36, 221],\n",
      "         [ 22, 183,  67]]],\n",
      "\n",
      "\n",
      "       [[[ 40, 182, 189],\n",
      "         [ 55, 194, 225],\n",
      "         [198,  87, 240],\n",
      "         ...,\n",
      "         [ 13, 145, 116],\n",
      "         [189,  35,  49],\n",
      "         [ 74, 199, 229]],\n",
      "\n",
      "        [[153,  73, 205],\n",
      "         [223,  77, 236],\n",
      "         [163, 231, 137],\n",
      "         ...,\n",
      "         [ 51,  31,  19],\n",
      "         [ 42,   4, 248],\n",
      "         [173, 231, 248]],\n",
      "\n",
      "        [[198, 186,  53],\n",
      "         [ 51,  78, 249],\n",
      "         [147,  27, 179],\n",
      "         ...,\n",
      "         [ 43,  79, 247],\n",
      "         [ 42, 114, 141],\n",
      "         [122,  65,  47]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[128,  90, 143],\n",
      "         [198,  43, 103],\n",
      "         [100,  36,  56],\n",
      "         ...,\n",
      "         [175, 250, 230],\n",
      "         [ 63, 149, 164],\n",
      "         [ 40,  72, 214]],\n",
      "\n",
      "        [[ 69, 221, 223],\n",
      "         [144, 164, 240],\n",
      "         [ 97, 224, 135],\n",
      "         ...,\n",
      "         [128,  42, 213],\n",
      "         [ 57, 111, 179],\n",
      "         [127, 254, 192]],\n",
      "\n",
      "        [[169,   1, 136],\n",
      "         [157, 122,  72],\n",
      "         [189, 226,  12],\n",
      "         ...,\n",
      "         [ 65,  58,  17],\n",
      "         [222,  38,  58],\n",
      "         [143,  78, 124]]]], dtype=uint8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:402: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:362: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if not dataset.common_metadata:\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:368: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  dataset_metadata_dict = dataset.common_metadata.metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/reader.py:418: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  metadata = dataset.metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  common_metadata = dataset.common_metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:278: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  sorted_pieces = sorted(dataset.pieces, key=attrgetter('path'))\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:146: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader\n",
    "\n",
    "with make_reader('file:///tmp/hello_world_dataset') as reader:\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea107f1-c79b-43f4-b942-d2eb578d00f4",
   "metadata": {},
   "source": [
    "## Tensorflow API\n",
    "To hookup the reader into a tensorflow graph, you can use the tf_tensors function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b67ed0-a38b-4b0a-bd23-36e6e1d93e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
      "  self._filesystem = pyarrow.localfs\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:402: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  dataset = pq.ParquetDataset(path_or_paths, filesystem=fs, validate_schema=False, metadata_nthreads=10)\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:362: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  if not dataset.common_metadata:\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:368: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  dataset_metadata_dict = dataset.common_metadata.metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/reader.py:418: FutureWarning: Specifying the 'metadata_nthreads' argument is deprecated as of pyarrow 8.0.0, and the argument will be removed in a future version\n",
      "  self.dataset = pq.ParquetDataset(dataset_path, filesystem=pyarrow_filesystem,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:253: FutureWarning: 'ParquetDataset.metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  metadata = dataset.metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:254: FutureWarning: 'ParquetDataset.common_metadata' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  common_metadata = dataset.common_metadata\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:278: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.fragments' attribute instead.\n",
      "  sorted_pieces = sorted(dataset.pieces, key=attrgetter('path'))\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/etl/dataset_metadata.py:288: FutureWarning: ParquetDatasetPiece is deprecated as of pyarrow 5.0.0 and will be removed in a future version.\n",
      "  rowgroups.append(pq.ParquetDatasetPiece(piece.path, open_file_func=dataset.fs.open, row_group=row_group,\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:146: FutureWarning: 'ParquetDataset.fs' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.filesystem' attribute instead.\n",
      "  parquet_file = ParquetFile(self._dataset.fs.open(piece.path))\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:182: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  partitions = self._dataset.partitions\n",
      "/opt/conda/lib/python3.10/site-packages/petastorm/py_dict_reader_worker.py:267: FutureWarning: 'ParquetDataset.partitions' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Specify 'use_legacy_dataset=False' while constructing the ParquetDataset, and then use the '.partitioning' attribute instead.\n",
      "  data_frame = piece.read(columns=column_names, partitions=self._dataset.partitions).to_pandas(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m make_reader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile:///tmp/hello_world_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m      4\u001b[0m     row_tensors \u001b[38;5;241m=\u001b[39m tf_tensors(reader)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28mprint\u001b[39m(session\u001b[38;5;241m.\u001b[39mrun(row_tensors))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49490)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from petastorm.tf_utils import tf_tensors\n",
    "import tensorflow as tf\n",
    "with make_reader('file:///tmp/hello_world_dataset') as reader:\n",
    "    row_tensors = tf_tensors(reader)\n",
    "    with tf.Session() as session:\n",
    "        for _ in range(3):\n",
    "            print(session.run(row_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf600d29-fd09-4af0-b823-d935e0f15151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
